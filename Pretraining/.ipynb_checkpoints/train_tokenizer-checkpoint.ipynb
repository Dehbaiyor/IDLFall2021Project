{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-kkz81OY6xH"
   },
   "source": [
    "## 2. Train a tokenizer\n",
    "\n",
    "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000.\n",
    "\n",
    "We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:11:51.198782Z",
     "iopub.status.busy": "2021-11-08T11:11:51.198198Z",
     "iopub.status.idle": "2021-11-08T11:11:58.439Z",
     "shell.execute_reply": "2021-11-08T11:11:58.43808Z",
     "shell.execute_reply.started": "2021-11-08T11:11:51.198686Z"
    },
    "id": "5duRggBRZKvP",
    "outputId": "39631fa1-4800-421b-f164-5c24fb0e6c95"
   },
   "outputs": [],
   "source": [
    "# We won't need TensorFlow here\n",
    "#!pip uninstall -y tensorflow\n",
    "# Install `transformers` from master\n",
    "!pip install transformers==2.11.0\n",
    "#!pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.11.0\n",
    "# tokenizers version at notebook update --- 0.8.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:11:58.441888Z",
     "iopub.status.busy": "2021-11-08T11:11:58.441436Z",
     "iopub.status.idle": "2021-11-08T11:11:58.461408Z",
     "shell.execute_reply": "2021-11-08T11:11:58.460713Z",
     "shell.execute_reply.started": "2021-11-08T11:11:58.441847Z"
    },
    "id": "IMnymRDLe0hi",
    "outputId": "0e561616-66d8-4c39-84dd-cf0ad4a19865"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\"../input/python-code-codenet/\").glob(\"*.csv\")]\n",
    "paths += [str(x) for x in Path(\"../input/lichess-elite/\").glob(\"*.csv\")]\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:11:58.463315Z",
     "iopub.status.busy": "2021-11-08T11:11:58.462609Z",
     "iopub.status.idle": "2021-11-08T11:21:55.672556Z",
     "shell.execute_reply": "2021-11-08T11:21:55.671577Z",
     "shell.execute_reply.started": "2021-11-08T11:11:58.463281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=10_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",a=\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ei7bqpRf1LH"
   },
   "source": [
    "Now let's save files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:24:51.242006Z",
     "iopub.status.busy": "2021-11-08T11:24:51.24173Z",
     "iopub.status.idle": "2021-11-08T11:24:51.923885Z",
     "shell.execute_reply": "2021-11-08T11:24:51.92304Z",
     "shell.execute_reply.started": "2021-11-08T11:24:51.241977Z"
    },
    "id": "EIS-irI0f32P",
    "outputId": "95defe36-d2cb-47d8-c12d-5a4e3aeb9c80"
   },
   "outputs": [],
   "source": [
    "!mkdir MixedTokens\n",
    "tokenizer.save(\"MixedTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:24:56.442104Z",
     "iopub.status.busy": "2021-11-08T11:24:56.441844Z",
     "iopub.status.idle": "2021-11-08T11:24:56.463893Z",
     "shell.execute_reply": "2021-11-08T11:24:56.463222Z",
     "shell.execute_reply.started": "2021-11-08T11:24:56.442074Z"
    },
    "id": "tKVWB8WShT-z"
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./MixedTokens/vocab.json\",\n",
    "    \"./MixedTokens/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:25:02.023265Z",
     "iopub.status.busy": "2021-11-08T11:25:02.022849Z",
     "iopub.status.idle": "2021-11-08T11:25:02.031583Z",
     "shell.execute_reply": "2021-11-08T11:25:02.030797Z",
     "shell.execute_reply.started": "2021-11-08T11:25:02.023224Z"
    },
    "id": "hO5M3vrAhcuj"
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:25:03.010006Z",
     "iopub.status.busy": "2021-11-08T11:25:03.009457Z",
     "iopub.status.idle": "2021-11-08T11:25:03.021518Z",
     "shell.execute_reply": "2021-11-08T11:25:03.020678Z",
     "shell.execute_reply.started": "2021-11-08T11:25:03.009969Z"
    },
    "id": "E3Ye27nchfzq",
    "outputId": "426ee056-b232-442f-fa93-8adb2c66cc06"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"def preprocessing(file_name):\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:25:36.016522Z",
     "iopub.status.busy": "2021-11-08T11:25:36.015783Z",
     "iopub.status.idle": "2021-11-08T11:25:36.023154Z",
     "shell.execute_reply": "2021-11-08T11:25:36.022363Z",
     "shell.execute_reply.started": "2021-11-08T11:25:36.016475Z"
    },
    "id": "X8ya5_7rhjKS",
    "outputId": "ded15113-3360-4d93-cf41-8b1470663d9a"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"e4 c5 d4\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQpUC_CDhnWW"
   },
   "source": [
    "## 3. Train a language model from scratch\n",
    "\n",
    "**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n",
    "\n",
    "> We‚Äôll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
    "\n",
    "As the model is BERT-like, we‚Äôll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:25:43.213318Z",
     "iopub.status.busy": "2021-11-08T11:25:43.212637Z",
     "iopub.status.idle": "2021-11-08T11:25:43.914265Z",
     "shell.execute_reply": "2021-11-08T11:25:43.913404Z",
     "shell.execute_reply.started": "2021-11-08T11:25:43.213281Z"
    },
    "id": "kD140sFjh0LQ",
    "outputId": "4feb635b-b95a-462f-c3d6-1e0349b7f6d5"
   },
   "outputs": [],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:25:46.090498Z",
     "iopub.status.busy": "2021-11-08T11:25:46.089834Z",
     "iopub.status.idle": "2021-11-08T11:25:46.535778Z",
     "shell.execute_reply": "2021-11-08T11:25:46.535046Z",
     "shell.execute_reply.started": "2021-11-08T11:25:46.090453Z"
    },
    "id": "VNZZs-r6iKAV",
    "outputId": "545e67b9-7d4c-41d1-806e-d43ddd369e4a"
   },
   "outputs": [],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0qQzgrBi1OX"
   },
   "source": [
    "### We'll define the following config for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:25:49.328143Z",
     "iopub.status.busy": "2021-11-08T11:25:49.327703Z",
     "iopub.status.idle": "2021-11-08T11:25:51.672865Z",
     "shell.execute_reply": "2021-11-08T11:25:51.672065Z",
     "shell.execute_reply.started": "2021-11-08T11:25:49.328105Z"
    },
    "id": "S4DB5xV8Dnsn"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaForMaskedLM, RobertaConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:29:34.803102Z",
     "iopub.status.busy": "2021-11-08T11:29:34.802481Z",
     "iopub.status.idle": "2021-11-08T11:29:34.809822Z",
     "shell.execute_reply": "2021-11-08T11:29:34.808873Z",
     "shell.execute_reply.started": "2021-11-08T11:29:34.803063Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:30:16.818621Z",
     "iopub.status.busy": "2021-11-08T11:30:16.818353Z",
     "iopub.status.idle": "2021-11-08T11:30:16.824546Z",
     "shell.execute_reply": "2021-11-08T11:30:16.823695Z",
     "shell.execute_reply.started": "2021-11-08T11:30:16.818591Z"
    },
    "id": "LTXXutqeDzPi"
   },
   "outputs": [],
   "source": [
    "#from transformers import RobertaConfig\n",
    "\n",
    "enc_config = RobertaConfig(\n",
    "    vocab_size=10_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_embeddings = 256,\n",
    "    num_attention_heads=32,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size = 256,\n",
    ")\n",
    "\n",
    "dec_config = RobertaConfig(\n",
    "    vocab_size=10_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=32,\n",
    "    num_embeddings = 256,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size = 256,\n",
    "    is_decoder = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAwQ82JiE5pi"
   },
   "source": [
    "Now let's re-create our tokenizer in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:30:19.282291Z",
     "iopub.status.busy": "2021-11-08T11:30:19.281655Z",
     "iopub.status.idle": "2021-11-08T11:30:19.309594Z",
     "shell.execute_reply": "2021-11-08T11:30:19.308749Z",
     "shell.execute_reply.started": "2021-11-08T11:30:19.28225Z"
    },
    "id": "4keFBUjQFOD1"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./MixedTokens\", max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yNCw-3hFv9h"
   },
   "source": [
    "Finally let's initialize our model.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:30:21.123188Z",
     "iopub.status.busy": "2021-11-08T11:30:21.122432Z",
     "iopub.status.idle": "2021-11-08T11:30:21.891222Z",
     "shell.execute_reply": "2021-11-08T11:30:21.890315Z",
     "shell.execute_reply.started": "2021-11-08T11:30:21.123142Z"
    },
    "id": "BzMqR-dzF4Ro",
    "outputId": "5d8b5b55-4c6d-4eb9-a81c-e162317dd4bf"
   },
   "outputs": [],
   "source": [
    "#from transformers import RobertaForMaskedLM\n",
    "#encoder = RobertaModel(config=enc_config)\n",
    "#decoder = RobertaForMaskedLM(config=dec_config)\n",
    "\n",
    "#model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "\n",
    "model = RobertaForMaskedLM(config = enc_config)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:30:23.002391Z",
     "iopub.status.busy": "2021-11-08T11:30:23.001649Z",
     "iopub.status.idle": "2021-11-08T11:30:23.009211Z",
     "shell.execute_reply": "2021-11-08T11:30:23.008464Z",
     "shell.execute_reply.started": "2021-11-08T11:30:23.002349Z"
    },
    "id": "jU6JhBSTKiaM",
    "outputId": "c1eae489-11e8-42f1-ad1b-ad6b6bfe42ed"
   },
   "outputs": [],
   "source": [
    "model.num_parameters()\n",
    "# => 180 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:30:25.830278Z",
     "iopub.status.busy": "2021-11-08T11:30:25.829634Z",
     "iopub.status.idle": "2021-11-08T11:30:26.574999Z",
     "shell.execute_reply": "2021-11-08T11:30:26.57421Z",
     "shell.execute_reply.started": "2021-11-08T11:30:25.830237Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtUHRMliOLM"
   },
   "source": [
    "### Now let's build our training Dataset\n",
    "\n",
    "We'll build our dataset by applying our tokenizer to our text file.\n",
    "\n",
    "Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:34:13.462461Z",
     "iopub.status.busy": "2021-11-08T11:34:13.462197Z"
    },
    "id": "GlvP_A-THEEl"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"../input/lichess-elite/lichess_elite.csv\",\n",
    "    block_size=64,\n",
    ")\n",
    "#from datasets import load_dataset\n",
    "#dataset = load_dataset('csv', data_files=paths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDLs73HcIHk5"
   },
   "source": [
    "Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n",
    "\n",
    "This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:32:35.522617Z",
     "iopub.status.busy": "2021-11-08T11:32:35.522274Z",
     "iopub.status.idle": "2021-11-08T11:32:35.532803Z",
     "shell.execute_reply": "2021-11-08T11:32:35.532042Z",
     "shell.execute_reply.started": "2021-11-08T11:32:35.522582Z"
    },
    "id": "zTgWPa9Dipk2"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2BIQKqjfHm"
   },
   "source": [
    "### Finally, we are all set to initialize our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:33:07.982502Z",
     "iopub.status.busy": "2021-11-08T11:33:07.982239Z",
     "iopub.status.idle": "2021-11-08T11:33:07.995674Z",
     "shell.execute_reply": "2021-11-08T11:33:07.994838Z",
     "shell.execute_reply.started": "2021-11-08T11:33:07.982472Z"
    },
    "id": "YpvnFFmZJD-N"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./EsperBERTo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6sASa36Nf-N"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T11:33:10.702839Z",
     "iopub.status.busy": "2021-11-08T11:33:10.702573Z",
     "iopub.status.idle": "2021-11-08T11:33:11.179867Z",
     "shell.execute_reply": "2021-11-08T11:33:11.179047Z",
     "shell.execute_reply.started": "2021-11-08T11:33:10.702809Z"
    },
    "id": "VmaHZXzmkNtJ",
    "outputId": "a19880cb-bcc6-4885-bf24-c2c6d0f56d1e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZkooHz1-_2h"
   },
   "source": [
    "#### üéâ Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDNgPls7_l13"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0caceCy_p1-"
   },
   "source": [
    "## 4. Check that the LM actually trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIQJ8ND_AEhl"
   },
   "source": [
    "Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n",
    "\n",
    "Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltXgXyCbAJLY"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIvgZ3S6AO0z",
    "outputId": "5f3d2f00-abdc-44a9-9c1b-75e3ec328576"
   },
   "outputs": [],
   "source": [
    "# The sun <mask>.\n",
    "# =>\n",
    "\n",
    "fill_mask(\"La suno <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0qCyyhNAWZi"
   },
   "source": [
    "Ok, simple syntax/grammar works. Let‚Äôs try a slightly more interesting prompt:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ9HSQxAAbme",
    "outputId": "aabfeedc-b1d0-4837-b01d-cd42726a5a3d"
   },
   "outputs": [],
   "source": [
    "fill_mask(\"Jen la komenco de bela <mask>.\")\n",
    "\n",
    "# This is the beginning of a beautiful <mask>.\n",
    "# =>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RsGaD1qAfLP"
   },
   "source": [
    "## 5. Share your model üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oESe8djApQw"
   },
   "source": [
    "Finally, when you have a nice model, please think about sharing it with the community:\n",
    "\n",
    "- upload your model using the CLI: `transformers-cli upload`\n",
    "- write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n",
    "    - a model description,\n",
    "    - training params (dataset, preprocessing, hyperparameters), \n",
    "    - evaluation results,\n",
    "    - intended uses & limitations\n",
    "    - whatever else is helpful! ü§ì\n",
    "\n",
    "### **TADA!**\n",
    "\n",
    "‚û°Ô∏è Your model has a page on http://huggingface.co/models and everyone can load it using `AutoModel.from_pretrained(\"username/model_name\")`.\n",
    "\n",
    "[![tb](https://huggingface.co/blog/assets/01_how-to-train/model_page.png)](https://huggingface.co/julien-c/EsperBERTo-small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw9ifsgqBI2o"
   },
   "source": [
    "If you want to take a look at models in different languages, check https://huggingface.co/models\n",
    "\n",
    "[![all models](https://huggingface.co/front/thumbnails/models.png)](https://huggingface.co/models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
