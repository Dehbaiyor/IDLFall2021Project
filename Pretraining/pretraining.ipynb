{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://huggingface.co/blog/how-to-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-kkz81OY6xH"
   },
   "source": [
    "## Train a tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"*.csv\")]\n",
    "paths = [paths[3], paths[6]]\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "] #+ chess_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=50_000, min_frequency=2, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"MixedTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tKVWB8WShT-z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./MixedTokens/vocab.json\",\n",
    "    \"./MixedTokens/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'e', '4', 'Ġc', '5', 'Ġd', '4', 'ĠQxf', '3', 'ĠO', '-', 'O', '</s>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"e4 c5 d4 Qxf3 O-O\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0qQzgrBi1OX"
   },
   "source": [
    "### We'll define the following config for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S4DB5xV8Dnsn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaForMaskedLM, RobertaConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LTXXutqeDzPi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import RobertaConfig\n",
    "\n",
    "enc_config = RobertaConfig(\n",
    "    vocab_size = 50_000,\n",
    "    max_position_embeddings = 514,\n",
    "    num_embeddings = 768,\n",
    "    num_attention_heads = 12,\n",
    "    num_hidden_layers = 12,\n",
    "    type_vocab_size = 1,\n",
    "    hidden_size = 768,\n",
    ")\n",
    "\n",
    "dec_config = RobertaConfig(\n",
    "    vocab_size = 50_000,\n",
    "    max_position_embeddings = 514,\n",
    "    num_embeddings = 768,\n",
    "    num_attention_heads = 32,\n",
    "    num_hidden_layers = 12,\n",
    "    type_vocab_size = 1,\n",
    "    hidden_size = 768,\n",
    "    is_decoder = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAwQ82JiE5pi"
   },
   "source": [
    "Now let's re-create our tokenizer in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4keFBUjQFOD1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./MixedTokens\", max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yNCw-3hFv9h"
   },
   "source": [
    "Finally let's initialize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BzMqR-dzF4Ro",
    "outputId": "5d8b5b55-4c6d-4eb9-a81c-e162317dd4bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import RobertaForMaskedLM\n",
    "#encoder = RobertaModel(config=enc_config)\n",
    "#decoder = RobertaForMaskedLM(config=dec_config)\n",
    "\n",
    "#model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "#model = RobertaForMaskedLM(config = enc_config)\n",
    "model = RobertaForMaskedLM.from_pretrained(\"./MixedTokens\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jU6JhBSTKiaM",
    "outputId": "c1eae489-11e8-42f1-ad1b-ad6b6bfe42ed",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125084240"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 125 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtUHRMliOLM"
   },
   "source": [
    "### Now let's build our training Dataset\n",
    "\n",
    "We'll build our dataset by applying our tokenizer to our text file.\n",
    "\n",
    "Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GlvP_A-THEEl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 10s, sys: 24.4 s, total: 8min 34s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"merged_large_bal.csv\",\n",
    "    block_size=64,\n",
    ")\n",
    "#from datasets import load_dataset\n",
    "#dataset = load_dataset('csv', data_files=paths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDLs73HcIHk5"
   },
   "source": [
    "Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n",
    "\n",
    "This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zTgWPa9Dipk2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2BIQKqjfHm"
   },
   "source": [
    "### Finally, we are all set to initialize our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YpvnFFmZJD-N",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./MixedTokens\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6sASa36Nf-N"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmaHZXzmkNtJ",
    "outputId": "a19880cb-bcc6-4885-bf24-c2c6d0f56d1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677e8bbf5e2540e48fc2266c54209999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f74215842fe4dcaa9285143c96c15f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/105421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.0006740362644195, \"learning_rate\": 4.9762855598030754e-05, \"epoch\": 0.004742888039384942, \"step\": 500}\n",
      "{\"loss\": 2.0613680160045624, \"learning_rate\": 4.9525711196061506e-05, \"epoch\": 0.009485776078769884, \"step\": 1000}\n",
      "{\"loss\": 2.0248699731826783, \"learning_rate\": 4.9288566794092265e-05, \"epoch\": 0.014228664118154826, \"step\": 1500}\n",
      "{\"loss\": 2.027987641572952, \"learning_rate\": 4.9051422392123016e-05, \"epoch\": 0.01897155215753977, \"step\": 2000}\n",
      "{\"loss\": 2.0233216965198517, \"learning_rate\": 4.881427799015377e-05, \"epoch\": 0.023714440196924712, \"step\": 2500}\n",
      "{\"loss\": 2.000885869979858, \"learning_rate\": 4.857713358818452e-05, \"epoch\": 0.028457328236309653, \"step\": 3000}\n",
      "{\"loss\": 2.0219379873275756, \"learning_rate\": 4.833998918621527e-05, \"epoch\": 0.0332002162756946, \"step\": 3500}\n",
      "{\"loss\": 1.9661591556072235, \"learning_rate\": 4.8102844784246024e-05, \"epoch\": 0.03794310431507954, \"step\": 4000}\n",
      "{\"loss\": 1.9859060139656066, \"learning_rate\": 4.786570038227678e-05, \"epoch\": 0.04268599235446448, \"step\": 4500}\n",
      "{\"loss\": 1.9819533774852753, \"learning_rate\": 4.7628555980307534e-05, \"epoch\": 0.047428880393849425, \"step\": 5000}\n",
      "{\"loss\": 1.9668902335166931, \"learning_rate\": 4.7391411578338286e-05, \"epoch\": 0.052171768433234365, \"step\": 5500}\n",
      "{\"loss\": 1.9521821398735046, \"learning_rate\": 4.715426717636904e-05, \"epoch\": 0.056914656472619306, \"step\": 6000}\n",
      "{\"loss\": 1.931064093351364, \"learning_rate\": 4.691712277439979e-05, \"epoch\": 0.061657544512004246, \"step\": 6500}\n",
      "{\"loss\": 1.9523448569774629, \"learning_rate\": 4.667997837243054e-05, \"epoch\": 0.0664004325513892, \"step\": 7000}\n",
      "{\"loss\": 1.9387626540660858, \"learning_rate\": 4.620568956849205e-05, \"epoch\": 0.07588620863015907, \"step\": 8000}\n",
      "{\"loss\": 1.954767413377762, \"learning_rate\": 4.59685451665228e-05, \"epoch\": 0.08062909666954401, \"step\": 8500}\n",
      "{\"loss\": 1.9201882181167602, \"learning_rate\": 4.5731400764553555e-05, \"epoch\": 0.08537198470892896, \"step\": 9000}\n",
      "{\"loss\": 1.9375789263248444, \"learning_rate\": 4.549425636258431e-05, \"epoch\": 0.09011487274831391, \"step\": 9500}\n",
      "{\"loss\": 1.9193868768215179, \"learning_rate\": 4.525711196061506e-05, \"epoch\": 0.09485776078769885, \"step\": 10000}\n",
      "{\"loss\": 1.9108455719947814, \"learning_rate\": 4.501996755864582e-05, \"epoch\": 0.09960064882708379, \"step\": 10500}\n",
      "{\"loss\": 1.894152711391449, \"learning_rate\": 4.478282315667657e-05, \"epoch\": 0.10434353686646873, \"step\": 11000}\n",
      "{\"loss\": 1.9007700424194336, \"learning_rate\": 4.454567875470732e-05, \"epoch\": 0.10908642490585367, \"step\": 11500}\n",
      "{\"loss\": 1.8922877621650696, \"learning_rate\": 4.430853435273807e-05, \"epoch\": 0.11382931294523861, \"step\": 12000}\n",
      "{\"loss\": 1.919462161064148, \"learning_rate\": 4.4071389950768824e-05, \"epoch\": 0.11857220098462355, \"step\": 12500}\n",
      "{\"loss\": 1.8723430092334747, \"learning_rate\": 4.3834245548799576e-05, \"epoch\": 0.12331508902400849, \"step\": 13000}\n",
      "{\"loss\": 1.8836030888557433, \"learning_rate\": 4.3597101146830335e-05, \"epoch\": 0.12805797706339345, \"step\": 13500}\n",
      "{\"loss\": 1.8984347453117372, \"learning_rate\": 4.3359956744861087e-05, \"epoch\": 0.1328008651027784, \"step\": 14000}\n",
      "{\"loss\": 1.8838190302848816, \"learning_rate\": 4.312281234289184e-05, \"epoch\": 0.13754375314216333, \"step\": 14500}\n",
      "{\"loss\": 1.860161882042885, \"learning_rate\": 4.2885667940922583e-05, \"epoch\": 0.14228664118154827, \"step\": 15000}\n",
      "{\"loss\": 1.8593536579608918, \"learning_rate\": 4.264852353895334e-05, \"epoch\": 0.1470295292209332, \"step\": 15500}\n",
      "{\"loss\": 1.8460169837474822, \"learning_rate\": 4.2411379136984094e-05, \"epoch\": 0.15177241726031815, \"step\": 16000}\n",
      "{\"loss\": 1.880118392944336, \"learning_rate\": 4.2174234735014846e-05, \"epoch\": 0.1565153052997031, \"step\": 16500}\n",
      "{\"loss\": 1.8538345563411713, \"learning_rate\": 4.19370903330456e-05, \"epoch\": 0.16125819333908803, \"step\": 17000}\n",
      "{\"loss\": 1.8525959768295288, \"learning_rate\": 4.169994593107635e-05, \"epoch\": 0.16600108137847297, \"step\": 17500}\n",
      "{\"loss\": 1.8725755145549774, \"learning_rate\": 4.14628015291071e-05, \"epoch\": 0.1707439694178579, \"step\": 18000}\n",
      "{\"loss\": 1.8316146323680877, \"learning_rate\": 4.122565712713786e-05, \"epoch\": 0.17548685745724285, \"step\": 18500}\n",
      "{\"loss\": 1.8325824806690216, \"learning_rate\": 4.098851272516861e-05, \"epoch\": 0.18022974549662782, \"step\": 19000}\n",
      "{\"loss\": 1.8357097721099853, \"learning_rate\": 4.075136832319936e-05, \"epoch\": 0.18497263353601276, \"step\": 19500}\n",
      "{\"loss\": 1.8343427469730378, \"learning_rate\": 4.0514223921230115e-05, \"epoch\": 0.1897155215753977, \"step\": 20000}\n",
      "{\"loss\": 1.8508931610584258, \"learning_rate\": 4.027707951926087e-05, \"epoch\": 0.19445840961478264, \"step\": 20500}\n",
      "{\"loss\": 1.8497747209072113, \"learning_rate\": 4.003993511729162e-05, \"epoch\": 0.19920129765416758, \"step\": 21000}\n",
      "{\"loss\": 1.8242527034282685, \"learning_rate\": 3.980279071532238e-05, \"epoch\": 0.20394418569355252, \"step\": 21500}\n",
      "{\"loss\": 1.8093946027755736, \"learning_rate\": 3.956564631335313e-05, \"epoch\": 0.20868707373293746, \"step\": 22000}\n",
      "{\"loss\": 1.7966858068704605, \"learning_rate\": 3.932850191138388e-05, \"epoch\": 0.2134299617723224, \"step\": 22500}\n",
      "{\"loss\": 1.8295527904033662, \"learning_rate\": 3.909135750941463e-05, \"epoch\": 0.21817284981170734, \"step\": 23000}\n",
      "{\"loss\": 1.827645501613617, \"learning_rate\": 3.8854213107445384e-05, \"epoch\": 0.22291573785109228, \"step\": 23500}\n",
      "{\"loss\": 1.8095535020828246, \"learning_rate\": 3.8617068705476136e-05, \"epoch\": 0.22765862589047722, \"step\": 24000}\n",
      "{\"loss\": 1.784815787434578, \"learning_rate\": 3.8379924303506895e-05, \"epoch\": 0.23240151392986216, \"step\": 24500}\n",
      "{\"loss\": 1.8037061016559601, \"learning_rate\": 3.8142779901537646e-05, \"epoch\": 0.2371444019692471, \"step\": 25000}\n",
      "{\"loss\": 1.8069115042686463, \"learning_rate\": 3.79056354995684e-05, \"epoch\": 0.24188729000863204, \"step\": 25500}\n",
      "{\"loss\": 1.7946445310115815, \"learning_rate\": 3.766849109759915e-05, \"epoch\": 0.24663017804801698, \"step\": 26000}\n",
      "{\"loss\": 1.7696524728536607, \"learning_rate\": 3.74313466956299e-05, \"epoch\": 0.25137306608740195, \"step\": 26500}\n",
      "{\"loss\": 1.7989020023345947, \"learning_rate\": 3.719420229366066e-05, \"epoch\": 0.2561159541267869, \"step\": 27000}\n",
      "{\"loss\": 1.8113341233730316, \"learning_rate\": 3.695705789169141e-05, \"epoch\": 0.26085884216617183, \"step\": 27500}\n",
      "{\"loss\": 1.7579419791698456, \"learning_rate\": 3.6719913489722164e-05, \"epoch\": 0.2656017302055568, \"step\": 28000}\n",
      "{\"loss\": 1.7644038298130036, \"learning_rate\": 3.553419147987593e-05, \"epoch\": 0.2893161704024815, \"step\": 30500}\n",
      "{\"loss\": 1.7527448657751084, \"learning_rate\": 3.529704707790668e-05, \"epoch\": 0.2940590584418664, \"step\": 31000}\n",
      "{\"loss\": 1.7642016625404358, \"learning_rate\": 3.5059902675937433e-05, \"epoch\": 0.29880194648125136, \"step\": 31500}\n",
      "{\"loss\": 1.7621962661743165, \"learning_rate\": 3.4822758273968185e-05, \"epoch\": 0.3035448345206363, \"step\": 32000}\n",
      "{\"loss\": 1.7575181745290755, \"learning_rate\": 3.458561387199894e-05, \"epoch\": 0.30828772256002124, \"step\": 32500}\n",
      "{\"loss\": 1.7582662830352782, \"learning_rate\": 3.4348469470029696e-05, \"epoch\": 0.3130306105994062, \"step\": 33000}\n",
      "{\"loss\": 1.7461180112361907, \"learning_rate\": 3.411132506806045e-05, \"epoch\": 0.3177734986387911, \"step\": 33500}\n",
      "{\"loss\": 1.7794525365829468, \"learning_rate\": 3.38741806660912e-05, \"epoch\": 0.32251638667817606, \"step\": 34000}\n",
      "{\"loss\": 1.7315767617225648, \"learning_rate\": 3.363703626412195e-05, \"epoch\": 0.327259274717561, \"step\": 34500}\n",
      "{\"loss\": 1.7366874958276748, \"learning_rate\": 3.33998918621527e-05, \"epoch\": 0.33200216275694594, \"step\": 35000}\n",
      "{\"loss\": 1.7536820454597473, \"learning_rate\": 3.3162747460183455e-05, \"epoch\": 0.3367450507963309, \"step\": 35500}\n",
      "{\"loss\": 1.742226691007614, \"learning_rate\": 3.292560305821421e-05, \"epoch\": 0.3414879388357158, \"step\": 36000}\n",
      "{\"loss\": 1.7249780604839324, \"learning_rate\": 3.2688458656244965e-05, \"epoch\": 0.34623082687510076, \"step\": 36500}\n",
      "{\"loss\": 1.734348683834076, \"learning_rate\": 3.245131425427572e-05, \"epoch\": 0.3509737149144857, \"step\": 37000}\n",
      "{\"loss\": 1.7388576256036758, \"learning_rate\": 3.221416985230647e-05, \"epoch\": 0.3557166029538707, \"step\": 37500}\n",
      "{\"loss\": 1.738105877518654, \"learning_rate\": 3.197702545033722e-05, \"epoch\": 0.36045949099325564, \"step\": 38000}\n",
      "{\"loss\": 1.7296917527914046, \"learning_rate\": 3.173988104836797e-05, \"epoch\": 0.3652023790326406, \"step\": 38500}\n",
      "{\"loss\": 1.7128417477607727, \"learning_rate\": 3.150273664639873e-05, \"epoch\": 0.3699452670720255, \"step\": 39000}\n",
      "{\"loss\": 1.7330021731853484, \"learning_rate\": 3.126559224442948e-05, \"epoch\": 0.37468815511141046, \"step\": 39500}\n",
      "{\"loss\": 1.7165363423824311, \"learning_rate\": 3.1028447842460234e-05, \"epoch\": 0.3794310431507954, \"step\": 40000}\n",
      "{\"loss\": 1.7080108242034913, \"learning_rate\": 3.0791303440490986e-05, \"epoch\": 0.38417393119018034, \"step\": 40500}\n",
      "{\"loss\": 1.6986992913484573, \"learning_rate\": 3.055415903852174e-05, \"epoch\": 0.3889168192295653, \"step\": 41000}\n",
      "{\"loss\": 1.7188207867145537, \"learning_rate\": 3.0317014636552493e-05, \"epoch\": 0.3936597072689502, \"step\": 41500}\n",
      "{\"loss\": 1.7156597187519074, \"learning_rate\": 3.0079870234583245e-05, \"epoch\": 0.39840259530833516, \"step\": 42000}\n",
      "{\"loss\": 1.7016550464630127, \"learning_rate\": 2.9842725832614e-05, \"epoch\": 0.4031454833477201, \"step\": 42500}\n",
      "{\"loss\": 1.6916651428937912, \"learning_rate\": 2.9605581430644752e-05, \"epoch\": 0.40788837138710504, \"step\": 43000}\n",
      "{\"loss\": 1.699642946958542, \"learning_rate\": 2.9368437028675504e-05, \"epoch\": 0.41263125942649, \"step\": 43500}\n",
      "{\"loss\": 1.680870451927185, \"learning_rate\": 2.913129262670626e-05, \"epoch\": 0.4173741474658749, \"step\": 44000}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZkooHz1-_2h"
   },
   "source": [
    "#### 🎉 Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QDNgPls7_l13"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./MixedTokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0caceCy_p1-",
    "tags": []
   },
   "source": [
    "## 4. Check that the LM actually trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIQJ8ND_AEhl"
   },
   "source": [
    "Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n",
    "\n",
    "Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ltXgXyCbAJLY"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./MixedTokens\",\n",
    "    tokenizer=\"./MixedTokens\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '<s> i += 1</s>', 'score': 0.7747066020965576, 'token': 328},\n",
       " {'sequence': '<s> i += 2</s>', 'score': 0.02622959017753601, 'token': 788},\n",
       " {'sequence': '<s> i += i</s>', 'score': 0.018232006579637527, 'token': 370},\n",
       " {'sequence': '<s> i += n</s>', 'score': 0.01438959315419197, 'token': 399},\n",
       " {'sequence': '<s> i += j</s>', 'score': 0.010179267264902592, 'token': 1218}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('i += <mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
