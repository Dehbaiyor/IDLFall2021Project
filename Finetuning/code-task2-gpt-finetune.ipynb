{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10769cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T20:20:23.794814Z",
     "iopub.status.busy": "2021-11-29T20:20:23.793305Z",
     "iopub.status.idle": "2021-11-29T20:20:37.545265Z",
     "shell.execute_reply": "2021-11-29T20:20:37.545740Z",
     "shell.execute_reply.started": "2021-11-29T20:10:11.829120Z"
    },
    "papermill": {
     "duration": 13.769525,
     "end_time": "2021-11-29T20:20:37.546067",
     "exception": false,
     "start_time": "2021-11-29T20:20:23.776542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aitextgen\r\n",
      "  Downloading aitextgen-0.5.2.tar.gz (572 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 572 kB 515 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from aitextgen) (4.5.1)\r\n",
      "Collecting fire>=0.3.0\r\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 87 kB 4.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pytorch-lightning>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from aitextgen) (1.4.4)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from aitextgen) (1.9.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire>=0.3.0->aitextgen) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire>=0.3.0->aitextgen) (1.1.0)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (2.6.0)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (2021.10.1)\r\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (0.18.2)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (0.5.0)\r\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (0.3.1)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (21.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (5.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (1.19.5)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (3.10.0.2)\r\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.3.1->aitextgen) (4.62.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2.25.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (3.7.4.post0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=17.0->pytorch-lightning>=1.3.1->aitextgen) (2.4.7)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.38.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.14.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.3.4)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.35.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.4.6)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.19.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.6.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (2.0.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.8.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (58.0.4)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.37.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (4.7.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.2.7)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (4.2.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (4.8.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (0.4.8)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (2021.10.8)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (4.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.1.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.5.1->aitextgen) (2021.8.28)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=4.5.1->aitextgen) (0.0.46)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.5.1->aitextgen) (3.0.12)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.5.1->aitextgen) (0.10.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (1.6.3)\r\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (3.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (21.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.1->aitextgen) (5.1.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.1->aitextgen) (3.5.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.5.1->aitextgen) (8.0.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.5.1->aitextgen) (1.0.1)\r\n",
      "Building wheels for collected packages: aitextgen, fire\r\n",
      "  Building wheel for aitextgen (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for aitextgen: filename=aitextgen-0.5.2-py3-none-any.whl size=575905 sha256=eb7e62839094ffbd18eb819788f62bda823f11e2a2399496121f0b9ed0ceb9d2\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/e2/74/46c887b0989a51a7acee0c09551a3ae9d34b939fb4bea404a0\r\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=2a466c35c1c6fa42c8e80ad8d18255d04fb7f6e7287bc25707ddf21118b08b3a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\r\n",
      "Successfully built aitextgen fire\r\n",
      "Installing collected packages: fire, aitextgen\r\n",
      "Successfully installed aitextgen-0.5.2 fire-0.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a4427b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T20:20:37.593732Z",
     "iopub.status.busy": "2021-11-29T20:20:37.592974Z",
     "iopub.status.idle": "2021-11-29T20:20:44.315582Z",
     "shell.execute_reply": "2021-11-29T20:20:44.314664Z",
     "shell.execute_reply.started": "2021-11-29T20:10:25.214904Z"
    },
    "id": "KBkpRgBCBS2_",
    "outputId": "629b3068-c9a6-407a-ea09-98773a0c50bf",
    "papermill": {
     "duration": 6.748586,
     "end_time": "2021-11-29T20:20:44.315752",
     "exception": false,
     "start_time": "2021-11-29T20:20:37.567166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d677c5de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T20:20:44.357487Z",
     "iopub.status.busy": "2021-11-29T20:20:44.355900Z",
     "iopub.status.idle": "2021-11-29T20:20:44.358143Z",
     "shell.execute_reply": "2021-11-29T20:20:44.358552Z",
     "shell.execute_reply.started": "2021-11-29T20:10:32.717986Z"
    },
    "papermill": {
     "duration": 0.024284,
     "end_time": "2021-11-29T20:20:44.358680",
     "exception": false,
     "start_time": "2021-11-29T20:20:44.334396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45dc5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T20:20:44.399688Z",
     "iopub.status.busy": "2021-11-29T20:20:44.399040Z",
     "iopub.status.idle": "2021-11-29T20:20:51.762453Z",
     "shell.execute_reply": "2021-11-29T20:20:51.761498Z",
     "shell.execute_reply.started": "2021-11-29T20:11:14.537053Z"
    },
    "papermill": {
     "duration": 7.385777,
     "end_time": "2021-11-29T20:20:51.762619",
     "exception": false,
     "start_time": "2021-11-29T20:20:44.376842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ai = aitextgen(model_folder=\"../input/gpt-finetuned-model/gpt_code_task2_model/trained_model\",\n",
    "               tokenizer_file=\"../input/gpt-finetuned-model/gpt_code_task2_model/trained_model/aitextgen.tokenizer.json\",\n",
    "               to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec28988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T20:20:51.806052Z",
     "iopub.status.busy": "2021-11-29T20:20:51.805326Z",
     "iopub.status.idle": "2021-11-29T20:20:53.687821Z",
     "shell.execute_reply": "2021-11-29T20:20:53.687202Z",
     "shell.execute_reply.started": "2021-11-29T20:12:43.990424Z"
    },
    "papermill": {
     "duration": 1.906151,
     "end_time": "2021-11-29T20:20:53.687997",
     "exception": false,
     "start_time": "2021-11-29T20:20:51.781846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_path_type = self.filename\n",
      "        if self.email.path.isabs(self.filename):\n",
      "            # Already found\n",
      "            return validate_path(self.filename, self.filename)\n",
      "        else:\n",
      "            # Remove contents of the file\n",
      "            return validate_path(self.filename) Validates a file.\n"
     ]
    }
   ],
   "source": [
    "ai.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26e2a3",
   "metadata": {
    "id": "LdpZQXknFNY3",
    "papermill": {
     "duration": 0.01828,
     "end_time": "2021-11-29T20:20:53.725568",
     "exception": false,
     "start_time": "2021-11-29T20:20:53.707288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train GPT-2\n",
    "\n",
    "Important parameters for `train()`:\n",
    "\n",
    "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
    "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
    "- **`num_steps`**: Number of steps to train the model for.\n",
    "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
    "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
    "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
    "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. _Unlike finetuning, since you are using a small model, you can massively increase the batch size to normalize the training_.\n",
    "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
    "\n",
    "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
    "\n",
    "- **`learning_rate`**: Learning rate of the model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf2c1e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T20:20:53.767430Z",
     "iopub.status.busy": "2021-11-29T20:20:53.766902Z",
     "iopub.status.idle": "2021-11-30T01:18:42.508177Z",
     "shell.execute_reply": "2021-11-30T01:18:42.509603Z"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "18a0eb53-7de1-4a2f-c66e-fa4ac911530b",
    "papermill": {
     "duration": 17868.765808,
     "end_time": "2021-11-30T01:18:42.509857",
     "exception": false,
     "start_time": "2021-11-29T20:20:53.744049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941648d9a9164fa38448b8b208d23862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09510f6e26f5452da92e014ba6d20aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def update_host_client_host(host_id, host_id,\n",
      "                                                                             robuser_host_id,\n",
      "                                             driver_id,\n",
      "                                             server_url):\n",
      "    \"\"\"\n",
      "    \n",
      "    \"\"\"\n",
      "    # Update the host_id is created\n",
      "    host_id = host_id.upper()\n",
      "    # We need to download host to the driver.\n",
      "    host_id = host_id.lower()\n",
      "    if host_id == host_id:\n",
      "        host_id = host_id.upper()\n",
      "    # Update the host_id\n",
      "    host_id = host_id.upper()\n",
      "    # Update the server\n",
      "    host_id = host_id.upper()\n",
      "    robuser_host = host_id.upper()\n",
      "    host_ids = host\n",
      "==========\n",
      "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def empty_contacts(contacts, transform=False,\n",
      "                 suffix=None,\n",
      "                 remove=True):\n",
      "    \"\"\"\n",
      "    \n",
      "    \"\"\"\n",
      "\n",
      "    # Extend to based on every permanent to ender transform. \n",
      "    if transform:\n",
      "        transformations = [f.get(x) for x in transformations]\n",
      "        transformations = [x.contacts(x) for x in transformations]\n",
      "\n",
      "    # If the function is the end of the ender transformation is already\n",
      "    ender_manager = transformations.empty_transformation(\n",
      "        transformations, ender_manager=ender_manager,\n",
      "        remove=remove,\n",
      "        transformation\n",
      "==========\n",
      "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def get_all_custom_name(self, cam, key, depth):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "\n",
      "        if depth == 0:\n",
      "            return depth + 1\n",
      "\n",
      "        if cam.get_custom_name() == 'custom_name':\n",
      "            return depth\n",
      "\n",
      "        span_id = self.get_all_custom_name(cam.get_custom_name()))\n",
      "        span_id = self.get_custom_name(cam.get_custom_name(), key)\n",
      "        if span_id is None:\n",
      "            return span_id\n",
      "\n",
      "        def parse_catalogs(self, arg):\n",
      "            return catalogs(arg, span_id)\n",
      "\n",
      "        return self.get_all_custom_name(catalogs, depth) Returns a partial-catalogs that have been parsed.   \n",
      "==========\n",
      "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\n",
      "            self.project_id = '{0}/{1}'.format(\n",
      "                self.project_id,\n",
      "                self.project_id,\n",
      "                self.project_id,\n",
      "                self.project_id,\n",
      "                self.project_id\n",
      "            )\n",
      "        else:\n",
      "            self.project_id = self.project_id\n",
      "\n",
      "        return self.project_id Returns list of projects in a project.\n",
      "==========\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def get_message(self):\n",
      "        \"\"\"  \"\"\"\n",
      "        return AWSAGE_DISCAPRS.get(self.name) Get the message message on the given message.\n",
      "==========\n",
      "\u001b[1m6,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def from_to_file(self, filename=None, source=None, content=LEARCE_FILT_SIZE, **kwargs):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    base_url = os.path.join(os.path.dirname(baseurl, filename))\n",
      "    filename = './' + filename\n",
      "    if content:\n",
      "        # source_dir = content\n",
      "        rel_cmd = '.\\d'\n",
      "    else:\n",
      "        # filename arguments\n",
      "        rel_cmd = '/'\n",
      "    if content:\n",
      "        rel_cmd = content + '.content'\n",
      "    if content:\n",
      "        cmd = render_template(**kwargs)\n",
      "    if source_dir:\n",
      "        cmd = './'\n",
      "    if source_dir:\n",
      "        cmd = './' + source_dir\n",
      "    if content:\n",
      "        cmd = '{0}/'.format(render_\n",
      "==========\n",
      "\u001b[1m7,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m7,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " = '''\n",
      "    '''\n",
      "    '''\n",
      "    #\n",
      "    text = ''\n",
      "    text += '\\''\n",
      "    if '' in text:\n",
      "        text += '\\''\n",
      "        text += '\\''\n",
      "\n",
      "    # If we have there are not able very can before\n",
      "    text += '\\''\n",
      "    return text Get the `text` passed`.\n",
      "==========\n",
      "\u001b[1m8,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m8,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _setup_all_all_all_all_all_all_all_all_all_all_all_all, new_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all\n",
      "==========\n",
      "\u001b[1m9,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m9,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\n",
      "==========\n",
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def get_assignment(surface_id, title=\"LIMA\", timestamp=None, secret_dic=None, text=None,\n",
      "         surface_id=None, full=False):\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    if title is None:\n",
      "        title = \"Current LiMA\"\n",
      "        timestamp = int(timestamp)\n",
      "        text = title\n",
      "\n",
      "    if title == 'UUMA':\n",
      "        title = title + title + title + title\n",
      "        surface_id = id + title + surface_id\n",
      "        # Update UUMANGRESS LIMAGE\n",
      "        title = title + title + title\n",
      "        title = ti\n",
      "==========\n",
      "\u001b[1m11,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m11,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def get_options(self):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        options = {}\n",
      "        for option in self.options:\n",
      "            option = option.replace('-', '-')\n",
      "        options[option.replace('-', '-', option)] = option\n",
      "        options[options[option.replace('-', '-')] = option\n",
      "        options[option.replace('-', '-')] = option\n",
      "        options[option.replace('-', '-')] = option\n",
      "        options[option.replace('-', '-')] = options\n",
      "        options[option.replace('-', option)] = option\n",
      "        options[option.replace('-', '-')] = option\n",
      "        options[option.replace('-', option.replace('-', option))] = option\n",
      "        options[option.replace('-', option)] = option\n",
      "        options[\n",
      "==========\n",
      "\u001b[1m12,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m12,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def title(self, label, label):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        if label is not None:\n",
      "            return label\n",
      "        else:\n",
      "            self.label = label\n",
      "        self.label = label\n",
      "        self.label = label Return label to the label in label.\n",
      "==========\n",
      "\u001b[1m13,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m13,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".get(\n",
      "            \"ID\", self.id.get(\"id\")\n",
      "        )\n",
      "        return self.logger.handlers.get(\n",
      "            \"ADD\", self.id,\n",
      "            \"POST\",\n",
      "            \"POST\",\n",
      "            \"DOC\",\n",
      "            \"DOC\",\n",
      "            \"DOC\",\n",
      "            \"GET\",\n",
      "            \"GET\",\n",
      "            \"GET\",\n",
      "            \"GET\",\n",
      "            \"DOC\",\n",
      "            \"GET\",\n",
      "            \"GET\",\n",
      "            \"GET\",\n",
      "            \"GET\",\n",
      "        ):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        docker.call(\"SUBPOST\", self.id)\n",
      "        docker.call(\"SUBPOST\", self.id)\n",
      "        docker.call(\"SUBPOST\",\n",
      "==========\n",
      "\u001b[1m14,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m14,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _find_all_all_function_for_mockages(self):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if not self.all_selected:\n",
      "            return False\n",
      "        if not self.selected:\n",
      "            self.find_all()\n",
      "            self.filter_multi_function_for_mocks(self.find_all_function_for_mocks)\n",
      "        elif not self.find_all_function:\n",
      "            self.all_selected = False\n",
      "            self.all_selected.clear()\n",
      "            self.all_selected = True\n",
      "        else:\n",
      "            self.all_selected = False\n",
      "            self._find_all_selected = True Find a finished for mockage in the list of functions.\n",
      "==========\n",
      "\u001b[1m15,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m15,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ": def check_many_table_table(self, values):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if not isinstance(values, tuple):\n",
      "            return values\n",
      "        else:\n",
      "            return self.tabletable_table.get(values) Check if values is only being specified in the list of tables\n",
      "==========\n",
      "\u001b[1m16,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m16,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ",\n",
      "        self.context_type: str)\n",
      "        try:\n",
      "            self.missing_identity = self.get_missing_identity\n",
      "        except KeyError:\n",
      "            return self.context_type\n",
      "        else:\n",
      "            return self.context_type Returns the context of the missing identity.\n",
      "==========\n",
      "\u001b[1m17,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m17,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def remove_constraints(self):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        # bins the constraints if the constraints is not set,\n",
      "        # the constraints work,\n",
      "        # make sure necessary.\n",
      "        for constraint in self.constraints:\n",
      "            # remove all the constraints\n",
      "            constraint.set_constraint(constraint, dstraint)\n",
      "        # only default constraints\n",
      "        for constraint in self.constraints:\n",
      "            # pragma: no take\n",
      "            # pragma: no cover\n",
      "            constraint.set_constraint(constraint, dstraint)\n",
      "        self.remove_constraints(constraint, -1) Remove constraints from a constraint in the con\n",
      "==========\n",
      "\u001b[1m18,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m18,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def update(self,\n",
      "                   name,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   password=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=None,\n",
      "                   user_id=\n",
      "==========\n",
      "\u001b[1m19,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m19,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def remove_missing_from_missing_from_missing(missing, force_missing_from_missing_from_missing,\n",
      "                                      replace_missing_from_missing_from_missing_from_missing_from_missing_from_missing,\n",
      "                                      remove_unique_missing_fb_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing_from_missing,\n",
      "                                         remove\n",
      "==========\n",
      "\u001b[1m20,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m20,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def reset(self) -> bool) -> bool:\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        cls = self.cls.load(self)\n",
      "        rect = rectangle.get_rectangle(self, cls.name)\n",
      "        rectangle = rectangle.get_rectangle(self, cls.name)\n",
      "        if rectangle is not None:\n",
      "            self.reset()\n",
      "        return cls Resets underlying rectangle in the given API.\n",
      "==========\n",
      "\u001b[1m21,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m21,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def get_value(self, value):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if self.sys_version <= 0 and not self.exclude_value:\n",
      "            return value\n",
      "\n",
      "        return self._delete_value(value) Return value for a value zone has a value of the value\n",
      "==========\n",
      "\u001b[1m22,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m22,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".append(cmd)\n",
      "    except AssertionError:\n",
      "        pass\n",
      "\n",
      "    if args.print_exc() in ['nonzero', 'print_exc']:\n",
      "        log.debug('print_exc() for opening data %s in %s' % (args.print_exc(), args.print_exc()))\n",
      "        run_exc = open(args.print_exc(), 'w')\n",
      "\n",
      "    else:\n",
      "        log.debug('print_exc(): %s' % cmd)\n",
      "\n",
      "        return None\n",
      "\n",
      "    # Get the data flag\n",
      "    load_data_flag = dataflag.get_load()\n",
      "    # make sure there.\n",
      "    try:\n",
      "        data = copy.copy()\n",
      "    except (AssertionError, IOError):\n",
      "        log.debug(\"print_exc() exc() for opening data %s in %s....\" % copy.decode\n",
      "==========\n",
      "\u001b[1m23,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m23,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def get_input(self, image):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        result = self._get(\"{0}\", \"{1}\".format(image, image))\n",
      "        return result, self._get(result, \"{0}\".format(image)) Returns a single image where the input is input is very up the view.\n",
      "==========\n",
      "\u001b[1m24,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m24,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def create(self, class_id, class_id):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        # Sometimes the ID of the class\n",
      "        class_id = class_id\n",
      "\n",
      "        # If the class is None\n",
      "        if class_id is None:\n",
      "            msg = self._create_manager(class_id)\n",
      "            return None\n",
      "\n",
      "        # If the class has been set, it is found, write it\n",
      "        if class_id is None:\n",
      "            raise exc.ClassID(class_id=class_id)\n",
      "\n",
      "        def add_manager(class_id):\n",
      "            if self._manager is None:\n",
      "                raise exc.ClassID(class_id=class_id)\n",
      "            msg = self._create_manager(class_id=class_id)\n",
      "            msg = self._create_manager(class_id\n",
      "==========\n",
      "\u001b[1m25,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m25,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\n",
      "==========\n",
      "\u001b[1m26,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m26,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _validate_tz(self, value, carrier=None):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if isinstance(value, (float, int)):\n",
      "            if value in self.tz else None\n",
      "            return self.tz[value]\n",
      "        if isinstance(value, int):\n",
      "            return value\n",
      "        return self.tz[value] Undertzer this function for a rounding column,    attempt to less the value into an index.\n",
      "==========\n",
      "\u001b[1m27,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m27,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _source_helpers(self, helpers, services, app_version, action, **kwargs):\n",
      "        ''\n",
      "        version = ''\n",
      "        version_list = []\n",
      "        for version in versions:\n",
      "            version_list.append(version)\n",
      "        services = version_list.split('\\n')\n",
      "        for version in version_list:\n",
      "            version_list.append(version)\n",
      "        version_list = []\n",
      "        for version in version_list:\n",
      "            version_list.append(version['version'])\n",
      "        for version in version_list:\n",
      "            version_list.append(version)\n",
      "        version_list = version_list.split('\\n')\n",
      "        version_list.append(version_list)\n",
      "        version_list = [version['version']]\n",
      "        return version_list, version_list, version_list, version_list, version_list, version_list, version_list\n",
      "==========\n",
      "\u001b[1m28,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m28,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _get_missing_matrix(self, mh, trim):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if trim == 'trim':\n",
      "            return [int(i) for i in self._get_missing_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h)))),\n",
      "                                           self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matrix(self.m_h_matri\n",
      "==========\n",
      "\u001b[1m29,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m29,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def reduce_suffix_suffix(self, suffix):\n",
      "        \"\"\"\"\"\"\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix_suffix(suffix)\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "        suffix = self.suffix\n",
      "\n",
      "==========\n",
      "\u001b[1m30,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m30,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def process(self, ref, filename):\n",
      "        \"\"\"\n",
      "\n",
      "        \"\"\"\n",
      "        # Make a PickleDataset for the data corresponding.\n",
      "        if self.is_pull.exists():\n",
      "            # Write to old file, ignore the Write term.\n",
      "            filename.write(self.data.read())\n",
      "        self.data.flush()\n",
      "        if ref.file_to_io:\n",
      "            return\n",
      "        else:\n",
      "            filename = self.path_path.replace('\\\\.py'.join([filename], filename))\n",
      "            if filename is None:\n",
      "                del _read_term(filename)\n",
      "            else:\n",
      "                filename = filename\n",
      "        if filename.is_pull() and not filename:\n",
      "                return\n",
      "            if filename.is_pull():\n",
      "                self.log.error(\"Disking title for title for tit\n",
      "==========\n",
      "\u001b[1m31,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m31,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _set_properties(self, limit=None):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        if self.samulations_total <= limit:\n",
      "            ax = ax.print_axes()\n",
      "            self.samulations_total += 1\n",
      "\n",
      "        elif self.samulations_total <= limit:\n",
      "            ax.set_bine(self.samulations_total)\n",
      "            ax.set_bins(self.samulations_total)\n",
      "\n",
      "        elif self.samulations_total >= limit:\n",
      "            ax.set_bine(self.samulations_total)\n",
      "            self.samulations_total += 1\n",
      "            self.samulations_total += 1\n",
      "\n",
      "        elif self.samulations_total >= limit:\n",
      "            ax.set\n",
      "==========\n",
      "\u001b[1m32,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m32,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def ensure_closed(self, normalizer) -> Tuple[Dict[str, bool]]:\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if self.normalizer:\n",
      "            return self.elements\n",
      "        elif normalizer:\n",
      "            return self.elements\n",
      "        else:\n",
      "            return self.elements Return an element using a given normalizer\n",
      "==========\n",
      "\u001b[1m33,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m33,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def create_constants(self, model, all_parameters=False, parameters=None,\n",
      "                               parameter_values=None):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        if isinstance(params, dict):\n",
      "            result = (params, model)\n",
      "        else:\n",
      "            result = {}\n",
      "\n",
      "        if isinstance(params, list):\n",
      "            # Have callable callables implicitly as a list\n",
      "            #\n",
      "            # if all_parameters and parameter_values is a list, then get tuples-in\n",
      "            #\n",
      "            # Have callables lists\n",
      "            result = []\n",
      "            for parameter_values in all_parameters:\n",
      "                # Have callables lists\n",
      "                result.append(params)\n",
      "            result.append(result)\n",
      "        else:\n",
      "            result = {}\n",
      "\n",
      "        return result Creating unique by all model\n",
      "==========\n",
      "\u001b[1m34,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m34,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ", field_name))\n",
      "    return Field(field_name) The value of field value which will be included in the value of field\n",
      "==========\n",
      "\u001b[1m35,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m35,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _get_regex_breaks(self):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        headers = {\n",
      "            'A':'s',\n",
      "            'B': 'A',\n",
      "            'A': 'A',\n",
      "            'E': 'E',\n",
      "            'B': 'U',\n",
      "            'E': 'B',\n",
      "            'A': 'A',\n",
      "            'F': 'B',\n",
      "            'A': 'D',\n",
      "            'U': 'P',\n",
      "            'H': 'D',\n",
      "            'F': 'G',\n",
      "            'S': 'E',\n",
      "            'M': 'D',\n",
      "            'M': 'D',\n",
      "            'N': 'n',\n",
      "        }\n",
      "        return self.get_regex_breaks(self.url + '_regex_breaks', '/regex_breaks', '/regex_breaks') Returns the Regex prefex breaks as an A.\n",
      "==========\n",
      "\u001b[1m36,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m36,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "c def _parse_text(self, line):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if not self._valid_segment:\n",
      "            return\n",
      "\n",
      "        yield from text_types.validations.Must(self.segment_string)\n",
      "\n",
      "        pre_disp = []\n",
      "        for line in line:\n",
      "            if line.startswith(\"#\"):\n",
      "                continue\n",
      "            else:\n",
      "                continue\n",
      "            pre_disp.append(line)\n",
      "\n",
      "        pre_disp = self.segment_strings.get(\"disp\", \" \")\n",
      "\n",
      "        if pre_disp:\n",
      "            pre_disp = self.segment_strings.get(\"disp\", \" \")\n",
      "\n",
      "            if pre_disp:\n",
      "                pre_disp = self.segment_strings.get(\"disp\", \" \" \")\n",
      "\n",
      "                pre_disp = self.segment_strings.get(\"disp\", \" \")\n",
      "\n",
      "==========\n",
      "\u001b[1m37,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m37,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " = int(x)\n",
      "\n",
      "        if not issparse.is_invalid(x):\n",
      "            return\n",
      "\n",
      "        if not isinstance(x, 'issparse'):\n",
      "            return\n",
      "\n",
      "        if not isinstance(x, unicode):\n",
      "            return x\n",
      "\n",
      "        if not isinstance(x, bytes):\n",
      "            return x\n",
      "\n",
      "        if hasattr(x, 'is_invalid'):\n",
      "            return\n",
      "\n",
      "        if len(x) > 0:\n",
      "            return x\n",
      "\n",
      "    else:\n",
      "        return x Generic lower that must be a bytestring argument name for Issparse in an abinary  argument.\n",
      "==========\n",
      "\u001b[1m38,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m38,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " def default_requirements(def: DefaultOptional[str])\n",
      "        if hasattr(def,'requirements'):\n",
      "            default_type = default_type\n",
      "        if hasattr(def,'stack_type'):\n",
      "            def=DefaultOption(DefaultOption(exception=default, **kwargs)\n",
      "            if hasattr(def,'stack_type'):\n",
      "                default_type = default_type.stack_type\n",
      "            if hasattr(def,'stack_type'):\n",
      "                def.__stack_type() = DefaultOption(e.__class__, stack_type)\n",
      "            else:\n",
      "                def.__class__ = DefaultOption(e.__class__, stack_type)\n",
      "        return default_type\n",
      "    return default_requirements Default requirements Internal defaults.\n",
      "==========\n",
      "\u001b[1m39,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m39,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "channels['sequence_id'])\n",
      "\n",
      "        cleaning_id = ''\n",
      "        where_id = self._where_id_seq_seq_seq_seq_seq_seq_seq_seq_seq\n",
      "        if cleaning_id is not None:\n",
      "            where_id = self._where_id_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_seq_\n",
      "==========\n",
      "\u001b[1m40,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m40,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def mrefm_gen_gene_calm_list(self, mrefm, gene_depth=None):\n",
      "        \"\"\"\"\"\"\n",
      "        from.mmiko import (\n",
      "            self.mrefm_frem_address,\n",
      "           ...) as m_f:\n",
      "            return m_f.depth\n",
      "        return m_f.get_mrefm_set(self.mrefm_address) Return mrefm set of mrefm sets for the MREFM fields.\n",
      "==========\n",
      "\u001b[1m41,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m41,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "ucture_type = self.solution['type'].lower()\n",
      "\n",
      "        if self.image_class:\n",
      "            self.images.update({\n",
      "                'new_delete': self.image_class,\n",
      "                'image_class': self.image_class,\n",
      "                'email': self.email,\n",
      "            })\n",
      "        else:\n",
      "            self.images.update({'new_deleted': self.image_class}) Creates a saved image.\n",
      "==========\n",
      "\u001b[1m42,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m42,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _load(self):\n",
      "        \"\"\"\"\"\"\n",
      "        if self.key.is_direct():\n",
      "            # Simply reloading tests..\n",
      "            self.value = self.reload(self.value)\n",
      "            self.key = self.key\n",
      "            if self.reload.is_direct():\n",
      "                self.key.is_direct()\n",
      "\n",
      "        return self Reload a directly connect to the file.\n",
      "==========\n",
      "\u001b[1m43,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m43,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _session_to_scope_request(self, server_id):\n",
      "        \"\"\" \n",
      "        \"\"\"\n",
      "        # If all servers can't throw now\n",
      "        if server_id.parent_name not in self.server_name:\n",
      "            # Not all servers to be an server, see\n",
      "            return\n",
      "        # If the server is not a server, see any\n",
      "        if server_name.parent_name not in self.server_name.parent_name:\n",
      "            # Not all server_name is a server, see if there are any\n",
      "            # in a specific name, server it will be a built-in a server.\n",
      "            # If an server is a server is not a server is a server, so just used.\n",
      "==========\n",
      "\u001b[1m44,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m44,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "r_change_position.deepcopy(\n",
      "            way,\n",
      "            routing_deepcopy)\n",
      "\n",
      "        if not self.position:\n",
      "            raise ValueError(\n",
      "                \"Could not parse: %s\" % c)\n",
      "\n",
      "        position_position = self.position_position\n",
      "\n",
      "        if self.active_position:\n",
      "            return (\n",
      "                position_position.deepcopy(),\n",
      "                routing_deepcopy),\n",
      "                routing_deepcopy)\n",
      "\n",
      "        return (position_position.deepcopy(pipeline_position)),\n",
      "            routing_deepcopy),\n",
      "        return (position_position.deepcopy()) Parses a specific position and returns a specific position  and returns a list of specific positions.\n",
      "==========\n",
      "\u001b[1m45,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m45,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _________\"____\"\n",
      "    \"\"\"\n",
      "    if callable(callable_method):\n",
      "        return SubClass.from_dict(conf, callable_method)\n",
      "    return None Find specific callable method instance to the dictionary against a callable method.\n",
      "==========\n",
      "\u001b[1m46,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m46,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def remove_remove_remove(self, remove_remove_remove):\n",
      "        \"\"\"\"\n",
      "        if remove_remove is None:\n",
      "            remove_remove = remove_remove\n",
      "        remove_remove_remove = remove_remove\n",
      "        remove_remove_remove.remove(remove_remove)\n",
      "        remove_remove.remove(remove_remove)\n",
      "        if remove_remove is None:\n",
      "            remove_remove.remove(remove_remove) Remove remove remove remove remove remove remove.\n",
      "==========\n",
      "\u001b[1m47,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m47,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def update_broadcast(self, broadcast, clean=False):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        fm = Fm(self._broadcast_in_broadcast_functions(clean, clean, broadcast),\n",
      "                      clean, clean, clean, clean, clean, clean, clean, clean, clean,\n",
      "                      clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, clean, c\n",
      "==========\n",
      "\u001b[1m48,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m48,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def is_payload(self, payload, message):\n",
      "        \"\"\"\n",
      "        \n",
      "        \"\"\"\n",
      "        if not self.is_payload:\n",
      "            return\n",
      "        with self.blocking(payload, message) as tb:\n",
      "            try:\n",
      "                message = (\"Requesting content\", message,\n",
      "                          \"Closing content type: %s\" % str(self.content))\n",
      "                # When content is some, the way, so we also close files\n",
      "                # and we close the only, we receive them.\n",
      "                self.content = \"%s/payload\" % payload[0]\n",
      "                self.content = None\n",
      "                self.content = message + \"Request\"\n",
      "            except Exception:\n",
      "                pass\n",
      "                self.content = None\n",
      "                self.content = None\n",
      "                self.content = None\n",
      "                self.content\n",
      "==========\n",
      "\u001b[1m49,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m49,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def apply_managed_managed_managed_managed(self, managed_managed):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        if managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or managed_managed['managed'] is None or m\n",
      "==========\n",
      "\u001b[1m50,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m50,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "summarize: def _series_to_track_series(self, track_series_to_track_series):\n",
      "        \"\"\"\"\"\"\n",
      "        for track_series in track_series_to_track_series:\n",
      "            track_series = track_series\n",
      "        return track_series Return a list of track series in the given track_series_to_track_series.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "ai.train('../input/idl-project-code-encoder-set/encoder_code_to_docstring_train.csv',\n",
    "         line_by_line=True,\n",
    "         from_cache=False,\n",
    "         num_steps=50_000,\n",
    "         generate_every=1_000,\n",
    "         save_every=1_000,\n",
    "         save_gdrive=False,\n",
    "         learning_rate=1e-3,\n",
    "         batch_size=2,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ffc33",
   "metadata": {
    "papermill": {
     "duration": 0.05626,
     "end_time": "2021-11-30T01:18:42.634374",
     "exception": false,
     "start_time": "2021-11-30T01:18:42.578114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c48d9737",
   "metadata": {
    "id": "pel-uBULXO2L",
    "papermill": {
     "duration": 0.063201,
     "end_time": "2021-11-30T01:18:42.753855",
     "exception": false,
     "start_time": "2021-11-30T01:18:42.690654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Load a Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd46d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-27T21:35:56.672341Z",
     "iopub.status.busy": "2021-11-27T21:35:56.671546Z",
     "iopub.status.idle": "2021-11-27T21:35:59.547952Z",
     "shell.execute_reply": "2021-11-27T21:35:59.546868Z",
     "shell.execute_reply.started": "2021-11-27T21:35:56.672307Z"
    },
    "papermill": {
     "duration": 0.058466,
     "end_time": "2021-11-30T01:18:42.875103",
     "exception": false,
     "start_time": "2021-11-30T01:18:42.816637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ai = aitextgen(model_folder=\"../input/finetuned-gpt/trained_model\",\n",
    "               tokenizer_file=\"../input/model-tuned/aitextgen.tokenizer.json\",\n",
    "               to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb3d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T06:24:41.195071Z",
     "iopub.status.busy": "2021-11-29T06:24:41.194781Z",
     "iopub.status.idle": "2021-11-29T06:24:41.46519Z",
     "shell.execute_reply": "2021-11-29T06:24:41.464185Z",
     "shell.execute_reply.started": "2021-11-29T06:24:41.195037Z"
    },
    "papermill": {
     "duration": 0.05734,
     "end_time": "2021-11-30T01:18:42.989452",
     "exception": false,
     "start_time": "2021-11-30T01:18:42.932112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../input/idl-project-code-encoder-set/encoder_docstring_to_code_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0e2de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T06:24:45.354781Z",
     "iopub.status.busy": "2021-11-29T06:24:45.354219Z",
     "iopub.status.idle": "2021-11-29T06:24:45.367218Z",
     "shell.execute_reply": "2021-11-29T06:24:45.366205Z",
     "shell.execute_reply.started": "2021-11-29T06:24:45.35474Z"
    },
    "papermill": {
     "duration": 0.058091,
     "end_time": "2021-11-30T01:18:43.104420",
     "exception": false,
     "start_time": "2021-11-30T01:18:43.046329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "i = 100\n",
    "string = df.iloc[i]['input'][:df.iloc[i]['input'].index('def')]\n",
    "code = df.iloc[i]['input'][df.iloc[i]['input'].index('def'):]\n",
    "\n",
    "print(f'String: {string}')\n",
    "print()\n",
    "print(f'Code: {code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b02846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T06:25:58.10806Z",
     "iopub.status.busy": "2021-11-29T06:25:58.107773Z",
     "iopub.status.idle": "2021-11-29T06:26:03.938527Z",
     "shell.execute_reply": "2021-11-29T06:26:03.937692Z",
     "shell.execute_reply.started": "2021-11-29T06:25:58.108024Z"
    },
    "id": "4RNY6RBI9LmL",
    "outputId": "8579ed7e-14c8-4a76-ecad-ce7acd13b612",
    "papermill": {
     "duration": 0.056998,
     "end_time": "2021-11-30T01:18:43.220442",
     "exception": false,
     "start_time": "2021-11-30T01:18:43.163444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ai.generate(max_length = 512, \n",
    "            prompt = string, \n",
    "            top_k = 100, \n",
    "            top_p  = 0.9, \n",
    "            temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a45fa",
   "metadata": {
    "papermill": {
     "duration": 0.056083,
     "end_time": "2021-11-30T01:18:43.332731",
     "exception": false,
     "start_time": "2021-11-30T01:18:43.276648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343eba1",
   "metadata": {
    "papermill": {
     "duration": 0.056248,
     "end_time": "2021-11-30T01:18:43.445392",
     "exception": false,
     "start_time": "2021-11-30T01:18:43.389144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17910.105538,
   "end_time": "2021-11-30T01:18:46.239642",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-29T20:20:16.134104",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05107301bafc41f99d439d6af16b42cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "09510f6e26f5452da92e014ba6d20aeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a555dcf3267e41d8a387759beeff1118",
        "IPY_MODEL_6a0024a3f1ee4d418d1e7390019a8389",
        "IPY_MODEL_a2b59876f1124fe5bb80046b413e11e1"
       ],
       "layout": "IPY_MODEL_f96aa40312a84c0bb5e703489209d806"
      }
     },
     "22ebe2456f574a3c9357a0b419ede02a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4065ad9cc5e84626b6c23ee1eb720218": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50c30ce25fbb477095d863ed90544df3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a0024a3f1ee4d418d1e7390019a8389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d916ef4e5ebe43ba9aad7aa1ac93e67b",
       "max": 50000.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ec21c4e3f34b41d486d4f9e441d0b3db",
       "value": 50000.0
      }
     },
     "7384872b3ac341d1ab5478f7cf92b7d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "759ed1941b9f46559e0c7982b44b1ab5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bf87e01721847038109e4a8b2313139": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_05107301bafc41f99d439d6af16b42cc",
       "placeholder": "​",
       "style": "IPY_MODEL_22ebe2456f574a3c9357a0b419ede02a",
       "value": " 265734/265734 [02:49&lt;00:00, 1571.63it/s]"
      }
     },
     "941648d9a9164fa38448b8b208d23862": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cc39564a3ee5422c82d83f1f11e515b3",
        "IPY_MODEL_c0413d1544bb44b4974ea136d7122211",
        "IPY_MODEL_7bf87e01721847038109e4a8b2313139"
       ],
       "layout": "IPY_MODEL_fd96ede883eb42959ac5b1b9ab485405"
      }
     },
     "a2b59876f1124fe5bb80046b413e11e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4065ad9cc5e84626b6c23ee1eb720218",
       "placeholder": "​",
       "style": "IPY_MODEL_e6bb3c465fe84e3d9b39c29900403dbf",
       "value": " 50000/50000 [4:54:49&lt;00:00,  2.83it/s]"
      }
     },
     "a555dcf3267e41d8a387759beeff1118": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_50c30ce25fbb477095d863ed90544df3",
       "placeholder": "​",
       "style": "IPY_MODEL_f25e6ac1985e4b9f82dd61ade1eb9611",
       "value": "Loss: 1.470 — Avg: 1.463 — GPU Mem: 13597 MB: 100%"
      }
     },
     "b393f3974e4042dc92a310ed0a75c1ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0413d1544bb44b4974ea136d7122211": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b393f3974e4042dc92a310ed0a75c1ed",
       "max": 265734.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7384872b3ac341d1ab5478f7cf92b7d7",
       "value": 265734.0
      }
     },
     "cae3f44a01d9498ebffb7e0999bf183e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cc39564a3ee5422c82d83f1f11e515b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_759ed1941b9f46559e0c7982b44b1ab5",
       "placeholder": "​",
       "style": "IPY_MODEL_cae3f44a01d9498ebffb7e0999bf183e",
       "value": "100%"
      }
     },
     "d916ef4e5ebe43ba9aad7aa1ac93e67b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6bb3c465fe84e3d9b39c29900403dbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ec21c4e3f34b41d486d4f9e441d0b3db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f25e6ac1985e4b9f82dd61ade1eb9611": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f96aa40312a84c0bb5e703489209d806": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "fd96ede883eb42959ac5b1b9ab485405": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
