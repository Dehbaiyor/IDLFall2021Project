{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul52VwW2D-IX"
   },
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: dataset==1.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: alembic>=0.6.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from dataset==1.0.2) (1.7.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from dataset==1.0.2) (1.3.23)\n",
      "Requirement already satisfied: normality>=0.3.9 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from dataset==1.0.2) (2.2.5)\n",
      "Requirement already satisfied: six>=1.7.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from dataset==1.0.2) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from alembic>=0.6.2->dataset==1.0.2) (2.0.0)\n",
      "Requirement already satisfied: Mako in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from alembic>=0.6.2->dataset==1.0.2) (1.1.6)\n",
      "Requirement already satisfied: importlib-resources in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from alembic>=0.6.2->dataset==1.0.2) (5.4.0)\n",
      "Requirement already satisfied: chardet in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from normality>=0.3.9->dataset==1.0.2) (4.0.0)\n",
      "Requirement already satisfied: banal>=1.0.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from normality>=0.3.9->dataset==1.0.2) (1.0.6)\n",
      "Requirement already satisfied: text-unidecode in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from normality>=0.3.9->dataset==1.0.2) (1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->alembic>=0.6.2->dataset==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from Mako->alembic>=0.6.2->dataset==1.0.2) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/envs/pytorch_latest_p37/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.12.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (4.12.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (1.19.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (0.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (2020.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers==4.12.5) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers==4.12.5) (4.0.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/envs/pytorch_latest_p37/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset==1.0.2\n",
    "!pip install transformers==4.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 4.3 MB/s            \n",
      "\u001b[?25hCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.1.0-py3-none-any.whl (19 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 122.7 MB/s            \n",
      "\u001b[?25hCollecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (7.1.2)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (3.18.1)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
      "     |████████████████████████████████| 180 kB 120.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from wandb) (1.15.0)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "     |████████████████████████████████| 97 kB 13.3 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "     |████████████████████████████████| 63 kB 3.9 MB/s              \n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Collecting termcolor<2.0.0,>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: promise, subprocess32, termcolor\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=1112269a181e8bf38e576414fc4a718f9e917ff408dd6c61c22bed9fc62dc0ee\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=11b54471c581b3142da53aa5c1e80c3221d691b934510d0fd60535d1534b1514\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=8c6f3e16657b7e3e136ef0ff2fb5ff00a478547ae7c016c9ee7c33e40b2b8980\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built promise subprocess32 termcolor\n",
      "Installing collected packages: smmap, termcolor, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, promise, GitPython, docker-pycreds, configparser, wandb\n",
      "Successfully installed GitPython-3.1.24 configparser-5.1.0 docker-pycreds-0.4.0 gitdb-4.0.9 promise-2.3 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 termcolor-1.1.0 wandb-0.12.7 yaspin-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/anaconda3/envs/pytorch_latest_p37/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!wandb login xxx # My token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14177,
     "status": "ok",
     "timestamp": 1632734285417,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "IWWLMcr0Yosy",
    "outputId": "467d3f2b-ec16-4801-93fb-fd13f31b263c"
   },
   "outputs": [],
   "source": [
    "#Installation\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "#Tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "#Training\n",
    "# When using previous version of the library you need the following two lines\n",
    "#from seq2seq_trainer import Seq2SeqTrainer\n",
    "#from transformers import TrainingArguments\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "#Wandb\n",
    "import wandb\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRZ9PVHF7IiX"
   },
   "source": [
    "Define parameters for data location and model folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-BWLTkI7X49"
   },
   "source": [
    "Load the datafile with the product descriptions and names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1632734285420,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "GQ48JL6J7bXl",
    "outputId": "57b1bfe0-3e89-4f20-c238-b67d98a85721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Examples:  12746073\n",
      "Null Values\n",
      " input     0\n",
      "target    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from a CSV file\n",
    "df=pd.read_csv('cleaned_chess_evaluation.csv', usecols=['input','target'])\n",
    "print('Num Examples: ',len(df))\n",
    "print('Null Values\\n', df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1632734285421,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "TT86FAZX7gmP",
    "outputId": "651eeaba-2d60-43bf-f3b4-b22e42cfef87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chess eval: d4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chess eval: d4 d5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chess eval: d4 d5 Nc3</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chess eval: d4 d5 Nc3 c6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chess eval: d4 d5 Nc3 c6 e4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         input target\n",
       "0               chess eval: d4    0.2\n",
       "1            chess eval: d4 d5    0.2\n",
       "2        chess eval: d4 d5 Nc3   -0.2\n",
       "3     chess eval: d4 d5 Nc3 c6    0.4\n",
       "4  chess eval: d4 d5 Nc3 c6 e4    0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NK0vJg07tnR"
   },
   "source": [
    "## Split the data into train and validation dataset\n",
    "\n",
    "We split the dataset into a training dataset (90%) and a validation dataset (10%). To choose the examples, there is a sampling method to randomly extract the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1632734285422,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "2XiyTneX8B5j",
    "outputId": "59980423-6725-4e51-cb22-83bc623fadf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train dataset:  1274607\n",
      "Length Val dataset:  114715\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and validation\n",
    "# Defining the train size. So 90% of the data will be used for training and the rest will be used for validation. \n",
    "train_size = 0.1\n",
    "val_size = 0.01\n",
    "\n",
    "# Sampling 90% fo the rows from the dataset\n",
    "train_dataset=df.sample(frac=train_size,random_state = 42)\n",
    "\n",
    "# Reset the indexes\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "val_dataset=val_dataset.sample(frac=val_size,random_state = 42).reset_index(drop=True)\n",
    "\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "print('Length Train dataset: ', len(train_dataset))\n",
    "print('Length Val dataset: ', len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfSfu7QHE-C0"
   },
   "source": [
    "In the next section, we try to limit the number of examples to train on in order to reduce the cost and time for training during the experiments. When the model is ready to be trained, we must train on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NRy5w_McbLOD"
   },
   "outputs": [],
   "source": [
    "# To limit the training and validation dataset, for testing\n",
    "start = 0\n",
    "stop = 1\n",
    "\n",
    "# Create a Dataset from a pandas dataframe for training and validation\n",
    "train_data=Dataset.from_pandas(train_dataset) #[start*200_000:stop*200_000])\n",
    "val_data=Dataset.from_pandas(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v97UsVpOFs_S"
   },
   "source": [
    "# Create the encoder-decoder model from a pretrained RoBERTa model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyfvVUhC9GGO"
   },
   "source": [
    "## Setting the model and training parameters\n",
    "\n",
    "Now it is time to set the model and training parameters, they will be passed to the dataset generator and to the Trainer object in a latter section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eQ3oqvQN9LN_"
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 64  # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 2   # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 1    # number of epochs to train (default: 10)\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 512      # Max length for product description\n",
    "SUMMARY_LEN = 7      # Max length for product names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At8Jlrr1ldQy"
   },
   "source": [
    "## Load the trained tokenizer on our specific language\n",
    "As we mentioned previously, we have trained a tokenizer and a RoBERTa model from scratch using the Masked Language Modelling technique trying to focus our model on our specific task. Now we can configure our encoder-decoder using this pretrained model.\n",
    "\n",
    "The first step is loading the tokenizer we need to apply to generate our input and target tokens and transform them into a vector representation of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1632734285947,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "juEoejqqa2e3",
    "outputId": "a0d985a7-4480-4659-f27c-c32c08c0ca69"
   },
   "outputs": [],
   "source": [
    "# Loading the RoBERTa Tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('../MixedTokens',  max_len=MAX_LEN)\n",
    "# Setting the BOS and EOS token\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "9f538f9d8f334ef589bb03ed39c524a2",
      "b9199a2f38b743758229ff96587aade7",
      "17c8957d90b24023bd4f34b7e69dfaff",
      "ca3b225fe04042c1b11361926faf9c72",
      "9f95feb156ad47b4b196b497057603cf",
      "1aa4404546d048be9095ce36f9a7cab7",
      "c60e580c0ae7412a82f598ceaf162462",
      "a0c76b6082e5418bb1624dab8fd6f68a",
      "25764dd28af84378be5588b2c2d771d4",
      "0807f4e7748e4bf2b0ad2a8587ed4e5a",
      "bb44322fee4549318415abec9c8a8325",
      "9d3e9aff149d464f9cf51aad6a3463f4",
      "fdb49bb47e0b4156b1ba9a3cd2aa9caf",
      "8623ad7c0c594f0298c6ede3097fabc9",
      "e70c980936be422fb12f352b6fc0e4e0",
      "10b1cd1eb5054ea4a2dff02443080e0c",
      "bcab682a4dce4516b002f0272be5e5aa",
      "5580f604ab664d4ab28b567376804c91",
      "0fcbbcf6fc964f7f8d7fbf75a453cc68",
      "d047ad4add1e435886d586aebecaa2ee",
      "d07f9a9e090947b1aaaca87429108fb0",
      "cead868d99864f5e9162e2b4bcc922e0"
     ]
    },
    "executionInfo": {
     "elapsed": 5604,
     "status": "ok",
     "timestamp": 1632734291544,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "iElONCScGD9e",
    "outputId": "4ea7823d-9608-4d6f-e6bb-4c3fb3d23ff5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea9fe734f93485cb078c34bcac1f210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19916 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06e641cb23849f48d571dfff2d8a5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1793 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=TRAIN_BATCH_SIZE  # change to 16 for full training\n",
    "encoder_max_length=MAX_LEN\n",
    "decoder_max_length=SUMMARY_LEN\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # Tokenize the input and target data\n",
    "  inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"target\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch\n",
    "\n",
    "# Preprocessing the training data\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"input\", \"target\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "# Preprocessing the validation data\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"input\", \"target\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "# Shuffle the dataset when it is needed\n",
    "#dataset = dataset.shuffle(seed=42, buffer_size=10, reshuffle_each_iteration=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J6lVbwq-BGV"
   },
   "source": [
    "## Define the RoBERTa Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2017,
     "status": "ok",
     "timestamp": 1632734294116,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "r0NyMtoRbAun",
    "outputId": "7acdd998-44e0-494a-d9ca-817e7a6e1467",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../MixedTokens were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at ../MixedTokens and are newly initialized: ['roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  50000\n"
     ]
    }
   ],
   "source": [
    "# set encoder decoder tying to True\n",
    "pretrainedmodel_folder = '../MixedTokens'\n",
    "roberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(pretrainedmodel_folder, pretrainedmodel_folder, tie_encoder_decoder=True)\n",
    "\n",
    "# Show the vocab size to check it has been loaded\n",
    "print('Vocab Size: ',roberta_shared.config.encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uqWYGXiBcEKL"
   },
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "roberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "roberta_shared.config.eos_token_id = tokenizer.eos_token_id\n",
    "roberta_shared.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "roberta_shared.config.max_length = SUMMARY_LEN\n",
    "roberta_shared.config.early_stopping = True\n",
    "roberta_shared.config.no_repeat_ngram_size = 1\n",
    "roberta_shared.config.length_penalty = 2.0\n",
    "roberta_shared.config.repetition_penalty = 3.0\n",
    "roberta_shared.config.num_beams = 10\n",
    "roberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wZQuCK__KI0"
   },
   "source": [
    "# Training the encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SKIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KFWMEZ3McNzM"
   },
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i04hngq6IBGu",
    "tags": []
   },
   "source": [
    "## Create the Trainer\n",
    "\n",
    "Now it is time to set the training arguments: batch_size, training epochs, save the model, etc. And then we can instantiate a `Seq2SeqTrainer`, a subclass of the `Trainer`object we mentioned, selecting the model to train, the training arguments, the metrics computation, the train, and the evaluation datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set wandb first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=Enc_Dec_finetune.ipynb\n",
      "env: WANDB_PROJECT=idl-project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME=Enc_Dec_finetune.ipynb\n",
    "%env WANDB_PROJECT=idl-project\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2573,
     "status": "ok",
     "timestamp": 1632734334492,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "93qw0sFTPol2",
    "outputId": "369e4817-a235-4cb1-9906-bcfa7eab169f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 4\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='../MixedTokens',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    predict_with_generate=True,\n",
    "    #evaluate_during_training=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1_000,  \n",
    "    save_steps=1_000, \n",
    "    warmup_steps=10,  \n",
    "    #max_steps=1500, # delete for full training\n",
    "    num_train_epochs = TRAIN_EPOCHS, #TRAIN_EPOCHS\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    "    fp16=True, \n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-first-test\",\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=roberta_shared,\n",
    "    args=training_args,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-R2qJClIFvz"
   },
   "source": [
    "Now, we start training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "executionInfo": {
     "elapsed": 6326124,
     "status": "ok",
     "timestamp": 1632740664013,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "W3bT7HuWVBbW",
    "outputId": "4617d138-9cdc-498b-ac9e-22de767f6f52",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1274607\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 318652\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kgmann/idl-project/runs/26shiage\" target=\"_blank\">roberta-first-test</a></strong> to <a href=\"https://wandb.ai/kgmann/idl-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='298025' max='318652' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [298025/318652 23:43:12 < 1:38:30, 3.49 it/s, Epoch 0.94/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../MixedTokens/checkpoint-1000\n",
      "Configuration saved in ../MixedTokens/checkpoint-1000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-15000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-2000\n",
      "Configuration saved in ../MixedTokens/checkpoint-2000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-1000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-3000\n",
      "Configuration saved in ../MixedTokens/checkpoint-3000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-2000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-4000\n",
      "Configuration saved in ../MixedTokens/checkpoint-4000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-3000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-5000\n",
      "Configuration saved in ../MixedTokens/checkpoint-5000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-4000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-6000\n",
      "Configuration saved in ../MixedTokens/checkpoint-6000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-5000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-7000\n",
      "Configuration saved in ../MixedTokens/checkpoint-7000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-6000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-8000\n",
      "Configuration saved in ../MixedTokens/checkpoint-8000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-7000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-9000\n",
      "Configuration saved in ../MixedTokens/checkpoint-9000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-8000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-10000\n",
      "Configuration saved in ../MixedTokens/checkpoint-10000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-9000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-20000\n",
      "Configuration saved in ../MixedTokens/checkpoint-20000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-19000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-21000\n",
      "Configuration saved in ../MixedTokens/checkpoint-21000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-20000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-22000\n",
      "Configuration saved in ../MixedTokens/checkpoint-22000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-21000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-31000\n",
      "Configuration saved in ../MixedTokens/checkpoint-31000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-30000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-32000\n",
      "Configuration saved in ../MixedTokens/checkpoint-32000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-31000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-33000\n",
      "Configuration saved in ../MixedTokens/checkpoint-33000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-32000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-34000\n",
      "Configuration saved in ../MixedTokens/checkpoint-34000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-33000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-35000\n",
      "Configuration saved in ../MixedTokens/checkpoint-35000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-34000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-36000\n",
      "Configuration saved in ../MixedTokens/checkpoint-36000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-35000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-40000\n",
      "Configuration saved in ../MixedTokens/checkpoint-40000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-39000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-41000\n",
      "Configuration saved in ../MixedTokens/checkpoint-41000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-40000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-42000\n",
      "Configuration saved in ../MixedTokens/checkpoint-42000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-41000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-43000\n",
      "Configuration saved in ../MixedTokens/checkpoint-43000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-42000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-44000\n",
      "Configuration saved in ../MixedTokens/checkpoint-44000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-43000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-51000\n",
      "Configuration saved in ../MixedTokens/checkpoint-51000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-51000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-50000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-52000\n",
      "Configuration saved in ../MixedTokens/checkpoint-52000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-52000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-51000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-53000\n",
      "Configuration saved in ../MixedTokens/checkpoint-53000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-53000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-52000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-54000\n",
      "Configuration saved in ../MixedTokens/checkpoint-54000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-54000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-53000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-55000\n",
      "Configuration saved in ../MixedTokens/checkpoint-55000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-55000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-54000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-56000\n",
      "Configuration saved in ../MixedTokens/checkpoint-56000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-56000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-55000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-57000\n",
      "Configuration saved in ../MixedTokens/checkpoint-57000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-57000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-56000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-58000\n",
      "Configuration saved in ../MixedTokens/checkpoint-58000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-58000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-57000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-59000\n",
      "Configuration saved in ../MixedTokens/checkpoint-59000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-59000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-58000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-60000\n",
      "Configuration saved in ../MixedTokens/checkpoint-60000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-60000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-59000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-61000\n",
      "Configuration saved in ../MixedTokens/checkpoint-61000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-61000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-60000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-62000\n",
      "Configuration saved in ../MixedTokens/checkpoint-62000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-62000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-61000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-63000\n",
      "Configuration saved in ../MixedTokens/checkpoint-63000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-63000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-62000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-64000\n",
      "Configuration saved in ../MixedTokens/checkpoint-64000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-63000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-65000\n",
      "Configuration saved in ../MixedTokens/checkpoint-65000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-65000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-64000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-66000\n",
      "Configuration saved in ../MixedTokens/checkpoint-66000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-66000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-65000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-67000\n",
      "Configuration saved in ../MixedTokens/checkpoint-67000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-67000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-66000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-68000\n",
      "Configuration saved in ../MixedTokens/checkpoint-68000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-68000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-67000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-69000\n",
      "Configuration saved in ../MixedTokens/checkpoint-69000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-69000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-68000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-70000\n",
      "Configuration saved in ../MixedTokens/checkpoint-70000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-69000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-71000\n",
      "Configuration saved in ../MixedTokens/checkpoint-71000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-71000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-70000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-72000\n",
      "Configuration saved in ../MixedTokens/checkpoint-72000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-72000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-71000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-73000\n",
      "Configuration saved in ../MixedTokens/checkpoint-73000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-73000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-72000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-74000\n",
      "Configuration saved in ../MixedTokens/checkpoint-74000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-74000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-73000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-75000\n",
      "Configuration saved in ../MixedTokens/checkpoint-75000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-75000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-74000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-76000\n",
      "Configuration saved in ../MixedTokens/checkpoint-76000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-76000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-75000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-77000\n",
      "Configuration saved in ../MixedTokens/checkpoint-77000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-77000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-76000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-78000\n",
      "Configuration saved in ../MixedTokens/checkpoint-78000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-78000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-77000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-79000\n",
      "Configuration saved in ../MixedTokens/checkpoint-79000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-79000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-78000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-80000\n",
      "Configuration saved in ../MixedTokens/checkpoint-80000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-79000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-81000\n",
      "Configuration saved in ../MixedTokens/checkpoint-81000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-81000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-80000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-82000\n",
      "Configuration saved in ../MixedTokens/checkpoint-82000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-82000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-81000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-83000\n",
      "Configuration saved in ../MixedTokens/checkpoint-83000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-83000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-82000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-84000\n",
      "Configuration saved in ../MixedTokens/checkpoint-84000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-84000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-83000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-85000\n",
      "Configuration saved in ../MixedTokens/checkpoint-85000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-85000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-84000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-86000\n",
      "Configuration saved in ../MixedTokens/checkpoint-86000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-86000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-85000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-87000\n",
      "Configuration saved in ../MixedTokens/checkpoint-87000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-87000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-86000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-88000\n",
      "Configuration saved in ../MixedTokens/checkpoint-88000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-88000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-88000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-88000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-87000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-89000\n",
      "Configuration saved in ../MixedTokens/checkpoint-89000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-89000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-89000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-89000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-88000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-90000\n",
      "Configuration saved in ../MixedTokens/checkpoint-90000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-90000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-90000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-90000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-89000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-91000\n",
      "Configuration saved in ../MixedTokens/checkpoint-91000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-91000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-91000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-91000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-90000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-92000\n",
      "Configuration saved in ../MixedTokens/checkpoint-92000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-92000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-92000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-92000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-91000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-93000\n",
      "Configuration saved in ../MixedTokens/checkpoint-93000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-93000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-93000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-93000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-92000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-94000\n",
      "Configuration saved in ../MixedTokens/checkpoint-94000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-94000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-94000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-94000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-93000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-95000\n",
      "Configuration saved in ../MixedTokens/checkpoint-95000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-95000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-95000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-95000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-94000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-96000\n",
      "Configuration saved in ../MixedTokens/checkpoint-96000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-95000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-97000\n",
      "Configuration saved in ../MixedTokens/checkpoint-97000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-97000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-97000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-97000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-96000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-98000\n",
      "Configuration saved in ../MixedTokens/checkpoint-98000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-98000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-98000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-98000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-97000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-99000\n",
      "Configuration saved in ../MixedTokens/checkpoint-99000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-99000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-99000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-99000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-98000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-100000\n",
      "Configuration saved in ../MixedTokens/checkpoint-100000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-100000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-99000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-101000\n",
      "Configuration saved in ../MixedTokens/checkpoint-101000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-101000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-101000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-101000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-100000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-102000\n",
      "Configuration saved in ../MixedTokens/checkpoint-102000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-102000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-102000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-102000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-101000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-103000\n",
      "Configuration saved in ../MixedTokens/checkpoint-103000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-103000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-103000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-103000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-102000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-104000\n",
      "Configuration saved in ../MixedTokens/checkpoint-104000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-104000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-104000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-104000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-103000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-105000\n",
      "Configuration saved in ../MixedTokens/checkpoint-105000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-105000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-104000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-106000\n",
      "Configuration saved in ../MixedTokens/checkpoint-106000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-106000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-106000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-106000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-105000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-107000\n",
      "Configuration saved in ../MixedTokens/checkpoint-107000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-107000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-107000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-107000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-106000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-108000\n",
      "Configuration saved in ../MixedTokens/checkpoint-108000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-108000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-108000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-108000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-107000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-109000\n",
      "Configuration saved in ../MixedTokens/checkpoint-109000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-109000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-109000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-109000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-108000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-110000\n",
      "Configuration saved in ../MixedTokens/checkpoint-110000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-110000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-110000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-110000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-109000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-111000\n",
      "Configuration saved in ../MixedTokens/checkpoint-111000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-111000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-111000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-111000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-110000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-112000\n",
      "Configuration saved in ../MixedTokens/checkpoint-112000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-111000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-113000\n",
      "Configuration saved in ../MixedTokens/checkpoint-113000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-113000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-113000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-113000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-112000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-114000\n",
      "Configuration saved in ../MixedTokens/checkpoint-114000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-114000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-114000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-114000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-113000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-115000\n",
      "Configuration saved in ../MixedTokens/checkpoint-115000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-115000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-115000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-115000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-114000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-116000\n",
      "Configuration saved in ../MixedTokens/checkpoint-116000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-116000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-116000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-116000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-115000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-117000\n",
      "Configuration saved in ../MixedTokens/checkpoint-117000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-117000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-117000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-117000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-116000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-118000\n",
      "Configuration saved in ../MixedTokens/checkpoint-118000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-118000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-118000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-118000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-117000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-119000\n",
      "Configuration saved in ../MixedTokens/checkpoint-119000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-119000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-119000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-119000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-118000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-120000\n",
      "Configuration saved in ../MixedTokens/checkpoint-120000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-120000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-120000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-120000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-119000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-121000\n",
      "Configuration saved in ../MixedTokens/checkpoint-121000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-121000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-121000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-121000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-120000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-122000\n",
      "Configuration saved in ../MixedTokens/checkpoint-122000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-122000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-122000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-122000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-121000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-123000\n",
      "Configuration saved in ../MixedTokens/checkpoint-123000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-123000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-123000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-123000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-122000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-124000\n",
      "Configuration saved in ../MixedTokens/checkpoint-124000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-124000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-124000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-124000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-123000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-125000\n",
      "Configuration saved in ../MixedTokens/checkpoint-125000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-125000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-125000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-125000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-124000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-126000\n",
      "Configuration saved in ../MixedTokens/checkpoint-126000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-126000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-126000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-126000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-125000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-127000\n",
      "Configuration saved in ../MixedTokens/checkpoint-127000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-127000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-127000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-127000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-126000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-128000\n",
      "Configuration saved in ../MixedTokens/checkpoint-128000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-127000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-129000\n",
      "Configuration saved in ../MixedTokens/checkpoint-129000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-129000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-129000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-129000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-128000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-130000\n",
      "Configuration saved in ../MixedTokens/checkpoint-130000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-130000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-130000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-130000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-129000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-131000\n",
      "Configuration saved in ../MixedTokens/checkpoint-131000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-131000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-131000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-131000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-130000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-132000\n",
      "Configuration saved in ../MixedTokens/checkpoint-132000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-132000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-132000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-132000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-131000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-133000\n",
      "Configuration saved in ../MixedTokens/checkpoint-133000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-133000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-133000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-133000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-132000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-134000\n",
      "Configuration saved in ../MixedTokens/checkpoint-134000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-134000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-134000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-134000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-133000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-135000\n",
      "Configuration saved in ../MixedTokens/checkpoint-135000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-135000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-135000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-135000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-134000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-136000\n",
      "Configuration saved in ../MixedTokens/checkpoint-136000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-136000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-136000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-136000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-135000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-137000\n",
      "Configuration saved in ../MixedTokens/checkpoint-137000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-137000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-137000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-137000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-136000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-138000\n",
      "Configuration saved in ../MixedTokens/checkpoint-138000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-138000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-138000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-138000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-137000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-139000\n",
      "Configuration saved in ../MixedTokens/checkpoint-139000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-139000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-139000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-139000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-138000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-140000\n",
      "Configuration saved in ../MixedTokens/checkpoint-140000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-140000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-140000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-140000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-139000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-141000\n",
      "Configuration saved in ../MixedTokens/checkpoint-141000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-141000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-141000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-141000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-140000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-142000\n",
      "Configuration saved in ../MixedTokens/checkpoint-142000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-142000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-142000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-142000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-141000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-143000\n",
      "Configuration saved in ../MixedTokens/checkpoint-143000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-143000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-143000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-143000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-142000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-144000\n",
      "Configuration saved in ../MixedTokens/checkpoint-144000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-143000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-145000\n",
      "Configuration saved in ../MixedTokens/checkpoint-145000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-145000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-145000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-145000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-144000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-146000\n",
      "Configuration saved in ../MixedTokens/checkpoint-146000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-146000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-146000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-146000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-145000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-147000\n",
      "Configuration saved in ../MixedTokens/checkpoint-147000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-147000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-147000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-147000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-146000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-148000\n",
      "Configuration saved in ../MixedTokens/checkpoint-148000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-148000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-148000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-148000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-147000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-149000\n",
      "Configuration saved in ../MixedTokens/checkpoint-149000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-149000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-149000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-149000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-148000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-150000\n",
      "Configuration saved in ../MixedTokens/checkpoint-150000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-150000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-150000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-150000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-149000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-151000\n",
      "Configuration saved in ../MixedTokens/checkpoint-151000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-151000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-151000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-151000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-150000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-152000\n",
      "Configuration saved in ../MixedTokens/checkpoint-152000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-152000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-152000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-152000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-151000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-153000\n",
      "Configuration saved in ../MixedTokens/checkpoint-153000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-153000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-153000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-153000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-152000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-154000\n",
      "Configuration saved in ../MixedTokens/checkpoint-154000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-154000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-154000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-154000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-153000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-155000\n",
      "Configuration saved in ../MixedTokens/checkpoint-155000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-155000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-155000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-155000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-154000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-156000\n",
      "Configuration saved in ../MixedTokens/checkpoint-156000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-156000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-156000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-156000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-155000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-157000\n",
      "Configuration saved in ../MixedTokens/checkpoint-157000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-157000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-157000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-157000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-156000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-158000\n",
      "Configuration saved in ../MixedTokens/checkpoint-158000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-158000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-158000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-158000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-157000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-159000\n",
      "Configuration saved in ../MixedTokens/checkpoint-159000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-159000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-159000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-159000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-158000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-160000\n",
      "Configuration saved in ../MixedTokens/checkpoint-160000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-160000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-159000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-161000\n",
      "Configuration saved in ../MixedTokens/checkpoint-161000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-161000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-161000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-161000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-160000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-162000\n",
      "Configuration saved in ../MixedTokens/checkpoint-162000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-162000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-162000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-162000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-161000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-163000\n",
      "Configuration saved in ../MixedTokens/checkpoint-163000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-163000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-163000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-163000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-162000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-164000\n",
      "Configuration saved in ../MixedTokens/checkpoint-164000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-164000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-164000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-164000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-163000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-165000\n",
      "Configuration saved in ../MixedTokens/checkpoint-165000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-165000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-165000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-165000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-164000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-166000\n",
      "Configuration saved in ../MixedTokens/checkpoint-166000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-166000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-166000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-166000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-165000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-167000\n",
      "Configuration saved in ../MixedTokens/checkpoint-167000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-167000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-167000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-167000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-166000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-168000\n",
      "Configuration saved in ../MixedTokens/checkpoint-168000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-168000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-168000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-168000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-167000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-169000\n",
      "Configuration saved in ../MixedTokens/checkpoint-169000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-169000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-169000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-169000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-168000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-170000\n",
      "Configuration saved in ../MixedTokens/checkpoint-170000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-170000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-170000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-170000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-169000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-171000\n",
      "Configuration saved in ../MixedTokens/checkpoint-171000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-171000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-171000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-171000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-170000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-172000\n",
      "Configuration saved in ../MixedTokens/checkpoint-172000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-172000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-172000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-172000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-171000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-173000\n",
      "Configuration saved in ../MixedTokens/checkpoint-173000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-173000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-173000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-173000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-172000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-174000\n",
      "Configuration saved in ../MixedTokens/checkpoint-174000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-174000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-174000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-174000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-173000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-175000\n",
      "Configuration saved in ../MixedTokens/checkpoint-175000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-175000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-175000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-175000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-174000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-176000\n",
      "Configuration saved in ../MixedTokens/checkpoint-176000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-176000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-176000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-176000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-175000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-177000\n",
      "Configuration saved in ../MixedTokens/checkpoint-177000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-177000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-177000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-177000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-176000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-178000\n",
      "Configuration saved in ../MixedTokens/checkpoint-178000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-178000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-178000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-178000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-177000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-179000\n",
      "Configuration saved in ../MixedTokens/checkpoint-179000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-179000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-179000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-179000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-178000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-180000\n",
      "Configuration saved in ../MixedTokens/checkpoint-180000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-180000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-180000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-180000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-179000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-181000\n",
      "Configuration saved in ../MixedTokens/checkpoint-181000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-181000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-181000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-181000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-180000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-182000\n",
      "Configuration saved in ../MixedTokens/checkpoint-182000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-182000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-182000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-182000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-181000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-183000\n",
      "Configuration saved in ../MixedTokens/checkpoint-183000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-183000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-183000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-183000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-182000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-184000\n",
      "Configuration saved in ../MixedTokens/checkpoint-184000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-184000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-184000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-184000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-183000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-185000\n",
      "Configuration saved in ../MixedTokens/checkpoint-185000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-185000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-185000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-185000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-184000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-186000\n",
      "Configuration saved in ../MixedTokens/checkpoint-186000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-186000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-186000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-186000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-185000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-187000\n",
      "Configuration saved in ../MixedTokens/checkpoint-187000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-187000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-187000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-187000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-186000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-188000\n",
      "Configuration saved in ../MixedTokens/checkpoint-188000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-188000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-188000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-188000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-187000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-189000\n",
      "Configuration saved in ../MixedTokens/checkpoint-189000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-189000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-189000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-189000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-188000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-190000\n",
      "Configuration saved in ../MixedTokens/checkpoint-190000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-190000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-190000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-190000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-189000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-191000\n",
      "Configuration saved in ../MixedTokens/checkpoint-191000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-191000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-191000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-191000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-190000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-192000\n",
      "Configuration saved in ../MixedTokens/checkpoint-192000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-192000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-192000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-192000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-191000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-193000\n",
      "Configuration saved in ../MixedTokens/checkpoint-193000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-193000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-193000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-193000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-192000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-194000\n",
      "Configuration saved in ../MixedTokens/checkpoint-194000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-194000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-194000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-194000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-193000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-195000\n",
      "Configuration saved in ../MixedTokens/checkpoint-195000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-195000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-195000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-195000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-194000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-196000\n",
      "Configuration saved in ../MixedTokens/checkpoint-196000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-196000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-196000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-196000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-195000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-197000\n",
      "Configuration saved in ../MixedTokens/checkpoint-197000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-197000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-197000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-197000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-196000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-198000\n",
      "Configuration saved in ../MixedTokens/checkpoint-198000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-198000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-198000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-198000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-197000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-199000\n",
      "Configuration saved in ../MixedTokens/checkpoint-199000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-199000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-199000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-199000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-198000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-200000\n",
      "Configuration saved in ../MixedTokens/checkpoint-200000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-200000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-200000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-200000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-199000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-201000\n",
      "Configuration saved in ../MixedTokens/checkpoint-201000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-201000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-201000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-201000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-200000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-202000\n",
      "Configuration saved in ../MixedTokens/checkpoint-202000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-202000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-202000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-202000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-201000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-203000\n",
      "Configuration saved in ../MixedTokens/checkpoint-203000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-203000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-203000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-203000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-202000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-204000\n",
      "Configuration saved in ../MixedTokens/checkpoint-204000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-204000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-204000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-204000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-203000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-205000\n",
      "Configuration saved in ../MixedTokens/checkpoint-205000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-205000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-205000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-205000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-204000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-206000\n",
      "Configuration saved in ../MixedTokens/checkpoint-206000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-206000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-206000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-206000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-205000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-207000\n",
      "Configuration saved in ../MixedTokens/checkpoint-207000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-207000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-207000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-207000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-206000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-208000\n",
      "Configuration saved in ../MixedTokens/checkpoint-208000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-208000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-208000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-208000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-207000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-209000\n",
      "Configuration saved in ../MixedTokens/checkpoint-209000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-209000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-209000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-209000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-208000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-210000\n",
      "Configuration saved in ../MixedTokens/checkpoint-210000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-210000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-210000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-210000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-209000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-211000\n",
      "Configuration saved in ../MixedTokens/checkpoint-211000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-211000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-211000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-211000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-210000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-212000\n",
      "Configuration saved in ../MixedTokens/checkpoint-212000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-212000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-212000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-212000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-211000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-213000\n",
      "Configuration saved in ../MixedTokens/checkpoint-213000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-213000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-213000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-213000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-212000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-214000\n",
      "Configuration saved in ../MixedTokens/checkpoint-214000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-214000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-214000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-214000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-213000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-215000\n",
      "Configuration saved in ../MixedTokens/checkpoint-215000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-215000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-215000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-215000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-214000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-216000\n",
      "Configuration saved in ../MixedTokens/checkpoint-216000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-216000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-216000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-216000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-215000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-217000\n",
      "Configuration saved in ../MixedTokens/checkpoint-217000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-217000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-217000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-217000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-216000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-218000\n",
      "Configuration saved in ../MixedTokens/checkpoint-218000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-218000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-218000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-218000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-217000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-219000\n",
      "Configuration saved in ../MixedTokens/checkpoint-219000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-219000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-219000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-219000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-218000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-220000\n",
      "Configuration saved in ../MixedTokens/checkpoint-220000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-220000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-220000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-220000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-219000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-221000\n",
      "Configuration saved in ../MixedTokens/checkpoint-221000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-221000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-221000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-221000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-220000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-222000\n",
      "Configuration saved in ../MixedTokens/checkpoint-222000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-222000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-222000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-222000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-221000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-223000\n",
      "Configuration saved in ../MixedTokens/checkpoint-223000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-223000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-223000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-223000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-222000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-224000\n",
      "Configuration saved in ../MixedTokens/checkpoint-224000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-224000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-224000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-224000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-223000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-225000\n",
      "Configuration saved in ../MixedTokens/checkpoint-225000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-225000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-225000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-225000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-224000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-226000\n",
      "Configuration saved in ../MixedTokens/checkpoint-226000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-226000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-226000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-226000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-225000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-227000\n",
      "Configuration saved in ../MixedTokens/checkpoint-227000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-227000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-227000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-227000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-226000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-228000\n",
      "Configuration saved in ../MixedTokens/checkpoint-228000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-228000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-228000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-228000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-227000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-229000\n",
      "Configuration saved in ../MixedTokens/checkpoint-229000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-229000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-229000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-229000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-228000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-230000\n",
      "Configuration saved in ../MixedTokens/checkpoint-230000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-230000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-230000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-230000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-229000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-231000\n",
      "Configuration saved in ../MixedTokens/checkpoint-231000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-231000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-231000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-231000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-230000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-232000\n",
      "Configuration saved in ../MixedTokens/checkpoint-232000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-232000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-232000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-232000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-231000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-233000\n",
      "Configuration saved in ../MixedTokens/checkpoint-233000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-233000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-233000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-233000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-232000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-234000\n",
      "Configuration saved in ../MixedTokens/checkpoint-234000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-234000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-234000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-234000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-233000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-235000\n",
      "Configuration saved in ../MixedTokens/checkpoint-235000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-235000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-235000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-235000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-234000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-236000\n",
      "Configuration saved in ../MixedTokens/checkpoint-236000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-236000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-236000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-236000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-235000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-237000\n",
      "Configuration saved in ../MixedTokens/checkpoint-237000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-237000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-237000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-237000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-236000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-238000\n",
      "Configuration saved in ../MixedTokens/checkpoint-238000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-238000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-238000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-238000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-237000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-239000\n",
      "Configuration saved in ../MixedTokens/checkpoint-239000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-239000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-239000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-239000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-238000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-240000\n",
      "Configuration saved in ../MixedTokens/checkpoint-240000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-240000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-240000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-240000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-239000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-241000\n",
      "Configuration saved in ../MixedTokens/checkpoint-241000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-241000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-241000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-241000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-240000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-242000\n",
      "Configuration saved in ../MixedTokens/checkpoint-242000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-242000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-242000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-242000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-241000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-243000\n",
      "Configuration saved in ../MixedTokens/checkpoint-243000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-243000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-243000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-243000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-242000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-244000\n",
      "Configuration saved in ../MixedTokens/checkpoint-244000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-244000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-244000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-244000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-243000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-245000\n",
      "Configuration saved in ../MixedTokens/checkpoint-245000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-245000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-245000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-245000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-244000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-246000\n",
      "Configuration saved in ../MixedTokens/checkpoint-246000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-246000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-246000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-246000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-245000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-247000\n",
      "Configuration saved in ../MixedTokens/checkpoint-247000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-247000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-247000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-247000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-246000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-248000\n",
      "Configuration saved in ../MixedTokens/checkpoint-248000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-248000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-248000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-248000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-247000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-249000\n",
      "Configuration saved in ../MixedTokens/checkpoint-249000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-249000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-249000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-249000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-248000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-250000\n",
      "Configuration saved in ../MixedTokens/checkpoint-250000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-250000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-250000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-250000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-249000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-251000\n",
      "Configuration saved in ../MixedTokens/checkpoint-251000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-251000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-251000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-251000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-250000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-252000\n",
      "Configuration saved in ../MixedTokens/checkpoint-252000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-252000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-252000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-252000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-251000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-253000\n",
      "Configuration saved in ../MixedTokens/checkpoint-253000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-253000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-253000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-253000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-252000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-254000\n",
      "Configuration saved in ../MixedTokens/checkpoint-254000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-254000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-254000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-254000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-253000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-255000\n",
      "Configuration saved in ../MixedTokens/checkpoint-255000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-255000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-255000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-255000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-254000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-256000\n",
      "Configuration saved in ../MixedTokens/checkpoint-256000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-256000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-256000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-256000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-255000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-257000\n",
      "Configuration saved in ../MixedTokens/checkpoint-257000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-257000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-257000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-257000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-256000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-258000\n",
      "Configuration saved in ../MixedTokens/checkpoint-258000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-258000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-258000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-258000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-257000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-259000\n",
      "Configuration saved in ../MixedTokens/checkpoint-259000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-259000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-259000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-259000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-258000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-260000\n",
      "Configuration saved in ../MixedTokens/checkpoint-260000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-260000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-260000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-260000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-259000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-261000\n",
      "Configuration saved in ../MixedTokens/checkpoint-261000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-261000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-261000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-261000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-260000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-262000\n",
      "Configuration saved in ../MixedTokens/checkpoint-262000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-262000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-262000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-262000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-261000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-263000\n",
      "Configuration saved in ../MixedTokens/checkpoint-263000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-263000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-263000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-263000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-262000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-264000\n",
      "Configuration saved in ../MixedTokens/checkpoint-264000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-264000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-264000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-264000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-263000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-265000\n",
      "Configuration saved in ../MixedTokens/checkpoint-265000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-265000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-265000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-265000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-264000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-266000\n",
      "Configuration saved in ../MixedTokens/checkpoint-266000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-266000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-266000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-266000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-265000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-267000\n",
      "Configuration saved in ../MixedTokens/checkpoint-267000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-267000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-267000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-267000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-266000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-268000\n",
      "Configuration saved in ../MixedTokens/checkpoint-268000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-268000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-268000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-268000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-267000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-269000\n",
      "Configuration saved in ../MixedTokens/checkpoint-269000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-269000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-269000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-269000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-268000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-270000\n",
      "Configuration saved in ../MixedTokens/checkpoint-270000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-270000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-270000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-270000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-269000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-271000\n",
      "Configuration saved in ../MixedTokens/checkpoint-271000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-271000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-271000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-271000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-270000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-272000\n",
      "Configuration saved in ../MixedTokens/checkpoint-272000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-272000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-272000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-272000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-271000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-273000\n",
      "Configuration saved in ../MixedTokens/checkpoint-273000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-273000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-273000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-273000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-272000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-274000\n",
      "Configuration saved in ../MixedTokens/checkpoint-274000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-274000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-274000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-274000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-273000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-275000\n",
      "Configuration saved in ../MixedTokens/checkpoint-275000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-275000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-275000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-275000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-274000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-276000\n",
      "Configuration saved in ../MixedTokens/checkpoint-276000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-276000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-276000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-276000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-275000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-277000\n",
      "Configuration saved in ../MixedTokens/checkpoint-277000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-277000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-277000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-277000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-276000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-278000\n",
      "Configuration saved in ../MixedTokens/checkpoint-278000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-278000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-278000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-278000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-277000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-279000\n",
      "Configuration saved in ../MixedTokens/checkpoint-279000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-279000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-279000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-279000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-278000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-280000\n",
      "Configuration saved in ../MixedTokens/checkpoint-280000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-280000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-280000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-280000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-279000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-281000\n",
      "Configuration saved in ../MixedTokens/checkpoint-281000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-281000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-281000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-281000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-280000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-282000\n",
      "Configuration saved in ../MixedTokens/checkpoint-282000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-282000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-282000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-282000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-281000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-287000\n",
      "Configuration saved in ../MixedTokens/checkpoint-287000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-287000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-287000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-287000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-286000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-288000\n",
      "Configuration saved in ../MixedTokens/checkpoint-288000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-288000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-288000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-288000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-287000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-289000\n",
      "Configuration saved in ../MixedTokens/checkpoint-289000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-289000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-289000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-289000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-288000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-290000\n",
      "Configuration saved in ../MixedTokens/checkpoint-290000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-290000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-290000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-290000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-289000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-291000\n",
      "Configuration saved in ../MixedTokens/checkpoint-291000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-291000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-291000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-291000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-290000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-292000\n",
      "Configuration saved in ../MixedTokens/checkpoint-292000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-292000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-292000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-292000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-291000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-293000\n",
      "Configuration saved in ../MixedTokens/checkpoint-293000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-293000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-293000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-293000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-292000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-294000\n",
      "Configuration saved in ../MixedTokens/checkpoint-294000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-294000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-294000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-294000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-293000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-295000\n",
      "Configuration saved in ../MixedTokens/checkpoint-295000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-295000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-295000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-295000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-294000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-296000\n",
      "Configuration saved in ../MixedTokens/checkpoint-296000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-296000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-296000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-296000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-295000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-297000\n",
      "Configuration saved in ../MixedTokens/checkpoint-297000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-297000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-297000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-297000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-296000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to ../MixedTokens/checkpoint-298000\n",
      "Configuration saved in ../MixedTokens/checkpoint-298000/config.json\n",
      "Model weights saved in ../MixedTokens/checkpoint-298000/pytorch_model.bin\n",
      "tokenizer config file saved in ../MixedTokens/checkpoint-298000/tokenizer_config.json\n",
      "Special tokens file saved in ../MixedTokens/checkpoint-298000/special_tokens_map.json\n",
      "Deleting older checkpoint [../MixedTokens/checkpoint-297000] due to args.save_total_limit\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model, training and evaluating on the train dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3lRmRvlbFzN"
   },
   "source": [
    "Save the encoder-decoder model just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1541,
     "status": "ok",
     "timestamp": 1632741920852,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "7xVP7Vtwaz-w",
    "outputId": "2e6db8a7-b4e2-4fbf-daa9-f2961d2b64cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to FinetunedModel\n",
      "Configuration saved in FinetunedModel/config.json\n",
      "Model weights saved in FinetunedModel/pytorch_model.bin\n",
      "tokenizer config file saved in FinetunedModel/tokenizer_config.json\n",
      "Special tokens file saved in FinetunedModel/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Save the encoder-decoder model just trained\n",
    "trainer.save_model('FinetunedModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkGO0gtQGjbI"
   },
   "source": [
    "# Evaluate the model on the test dataset\n",
    "\n",
    "Once we have our model trained, we can use it to generate names for our products and check the result of our fine-tuning process on our objective task. \n",
    "\n",
    "We load a test dataset, a subset of our original dataset and delete rows containing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1632741925510,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "D727N4XHDhDn",
    "outputId": "c924cc3e-7566-4894-ccb5-0ca7b6f6f806"
   },
   "outputs": [],
   "source": [
    "# Load the dataset: sentence in english, sentence in spanish \n",
    "df=pd.read_csv('chess_data/23.csv')\n",
    "print('Num Examples: ',len(df))\n",
    "print('Null Values\\n', df.isna().sum())\n",
    "print(df.head(5))\n",
    "\n",
    "test_data=Dataset.from_pandas(df[:30])\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8B3MLGJw5uT"
   },
   "source": [
    "If you need to **restore the trained model from a checkpoint** run the next cell, selecting the folder where the checkpoint was saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6HLy-pqrVh7"
   },
   "source": [
    "checkpoint_path = os.path.abspath(os.path.join(model_folder,'checkpoint-3072'))\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx8HQRJqrCqN"
   },
   "source": [
    "Then we load the Tokenizer and the fine-tuned model from a saved version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3659,
     "status": "ok",
     "timestamp": 1612897968696,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "80cdTzPRrJPg",
    "outputId": "df5d278b-e627-45fb-ee8b-90ccbac1a3ae"
   },
   "outputs": [],
   "source": [
    "#Load the Tokenizer and the fine-tuned model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('FinetunedModel')\n",
    "model = EncoderDecoderModel.from_pretrained('FinetunedModel')\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oukYOQoSIm7q"
   },
   "source": [
    "In order to improve the results, we will define two methods to generate the text, using the Beam search decoding strategy and random sampling, and we will apply them and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W79b87IBcSG-"
   },
   "outputs": [],
   "source": [
    "# Generate the text without setting a decoding strategy\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    #outputs = roberta_shared.generate(input_ids, attention_mask=attention_mask)\n",
    "    outputs = roberta_shared.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RypfesHQnUsE"
   },
   "outputs": [],
   "source": [
    "# Generate a text using beams search\n",
    "def generate_summary_beam_search(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = roberta_shared.generate(input_ids, attention_mask=attention_mask,\n",
    "                                  num_beams=15,\n",
    "                                  repetition_penalty=3.0, \n",
    "                                  length_penalty=2.0, \n",
    "                                  num_return_sequences = 1\n",
    "    )\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Generate a text using beams search\n",
    "def generate_summary_topk(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = roberta_shared.generate(input_ids, attention_mask=attention_mask,\n",
    "                                  repetition_penalty=3.0, \n",
    "                                  length_penalty=2.0, \n",
    "                                  num_return_sequences = 1,\n",
    "                                  do_sample=True,\n",
    "                                  top_k=50, \n",
    "                                  top_p=0.95,\n",
    "\n",
    "    )\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfG402bqI3t2"
   },
   "source": [
    "Now, we can make predictions for the test dataset using Beam search strategy and top-k sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "027dce30a62a40ad9a7c9d0941c020c7",
      "e0e87c7d1bb94a1fa42467a101db1f61",
      "3b441a90c1b948348a5e718b2f20a382",
      "bb6f3e8b55c4472fbe4de30e13674f6e",
      "012b238bb08745b7a6d4ac419f2480c7",
      "d0dd958562304ef89c02216518d2273a",
      "aac6248693ab425db9e5879c9a1705d4",
      "8896aace47ba49efad33b80b68bf0c86",
      "0c3127e067f1411ca3efdd36b233cb10",
      "de898c4bfdf24aa5966b9c31e913b2f5",
      "243d56b7c05248af8cd275b4c03d5453",
      "18dc49750fad41d79c32696610feb651",
      "e0d70b350ce24b618d427907fdd72f9d",
      "426f35e25c5541f2822f3bc7744bb7b5",
      "1004f2e3fcdb429fafb138d869939a89",
      "b97b7461cf33481581ce7fe7d746ef3f",
      "f7476c58ef3b48f4948443679a1bdf47",
      "cd3465e5cd6240debfd29b2ecfb3bfe1",
      "88420b2c8e2341118d1222fc1c19466d",
      "5b7c69538d2d48fca6bb0e454b43a896",
      "ff5a6eb413484323948a15e358a54f78",
      "3e028a9817c746ca824b1a21e3fe6b22"
     ]
    },
    "executionInfo": {
     "elapsed": 202677,
     "status": "ok",
     "timestamp": 1632742143944,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "1XeMAIyxHnqq",
    "outputId": "57eb2600-29a9-4501-b598-b00e46b55cfc"
   },
   "outputs": [],
   "source": [
    "batch_size = TRAIN_BATCH_SIZE\n",
    "\n",
    "#results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"description\"])\n",
    "# Generate predictions using beam search\n",
    "results = test_data.map(generate_summary_beam_search, batched=True, batch_size=batch_size, remove_columns=[\"input\"])\n",
    "pred_str_bs = results[\"pred\"]\n",
    "# Generate predictions using top-k sampling\n",
    "results = test_data.map(generate_summary_topk, batched=True, batch_size=batch_size, remove_columns=[\"input\"])\n",
    "pred_str_topk = results[\"pred\"]\n",
    "\n",
    "#label_str = results[\"Summary\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4stzYN6ZzP-X"
   },
   "source": [
    "Now, we can see some results from our trained model to check its performance on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1632742156758,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "2uHlB8U6f4je",
    "outputId": "9f926162-6d84-40d6-b174-9ad9e4b02c3e"
   },
   "outputs": [],
   "source": [
    "#Show an example\n",
    "print(\"Moves: \",df['input'][1])\n",
    "print(\"Predicted using BS: \", pred_str_bs[1])\n",
    "print(\"Predicted using Top-K Sampling: \", pred_str_topk[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1632742168005,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -120
    },
    "id": "B60rzZ7UhQfp",
    "outputId": "c8095fa2-e911-425b-cadf-5aece2f65a0d"
   },
   "outputs": [],
   "source": [
    "#Show an example\n",
    "print(\"Moves: \",df['input'][10])\n",
    "print(\"Predicted using BS: \", pred_str_bs[10])\n",
    "print(\"Predicted using Top-K Sampling: \", pred_str_topk[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2kc3xwTTFXP"
   },
   "source": [
    "When more than one output are generated we need to join them on a single list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1612981617914,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "wIFmWu1Lu9xb",
    "outputId": "ae4e571b-9e5d-4ea5-f59c-adda1d2f98b4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds=np.reshape(pred_str, (-1, 3))\n",
    "print('Predictions Shape: ',preds.shape)\n",
    "predictions = [','.join(p) for p in preds]\n",
    "print('Num predictions: ', len(predictions),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1612981655396,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "VxYXAvjDvYjV",
    "outputId": "5e5da222-2662-41a7-cb2c-41d829acf210"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdpdHJ28ThJW"
   },
   "source": [
    "Save the predictions to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1614429421680,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "RYBRQ7o8mh0V",
    "outputId": "696e3926-07aa-4dac-d035-261c2d5aae2c"
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({'name':pred_str})\n",
    "final_df.to_csv(outputfile_path, index=False)\n",
    "print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMRYwOlmiDqw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "W7Sk2Mv-3WYJ"
   ],
   "name": "RoBERTa Encoder Decoder MLM FineTuned for Text generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "012b238bb08745b7a6d4ac419f2480c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_243d56b7c05248af8cd275b4c03d5453",
      "placeholder": "​",
      "style": "IPY_MODEL_de898c4bfdf24aa5966b9c31e913b2f5",
      "value": " 91/91 [01:54&lt;00:00,  1.11ba/s]"
     }
    },
    "027dce30a62a40ad9a7c9d0941c020c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b441a90c1b948348a5e718b2f20a382",
       "IPY_MODEL_bb6f3e8b55c4472fbe4de30e13674f6e",
       "IPY_MODEL_012b238bb08745b7a6d4ac419f2480c7"
      ],
      "layout": "IPY_MODEL_e0e87c7d1bb94a1fa42467a101db1f61"
     }
    },
    "0807f4e7748e4bf2b0ad2a8587ed4e5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c3127e067f1411ca3efdd36b233cb10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fcbbcf6fc964f7f8d7fbf75a453cc68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1004f2e3fcdb429fafb138d869939a89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b7c69538d2d48fca6bb0e454b43a896",
      "max": 91,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88420b2c8e2341118d1222fc1c19466d",
      "value": 91
     }
    },
    "10b1cd1eb5054ea4a2dff02443080e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cead868d99864f5e9162e2b4bcc922e0",
      "placeholder": "​",
      "style": "IPY_MODEL_d07f9a9e090947b1aaaca87429108fb0",
      "value": " 198/198 [00:00&lt;00:00, 293.26ba/s]"
     }
    },
    "17c8957d90b24023bd4f34b7e69dfaff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c60e580c0ae7412a82f598ceaf162462",
      "placeholder": "​",
      "style": "IPY_MODEL_1aa4404546d048be9095ce36f9a7cab7",
      "value": "100%"
     }
    },
    "18dc49750fad41d79c32696610feb651": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_426f35e25c5541f2822f3bc7744bb7b5",
       "IPY_MODEL_1004f2e3fcdb429fafb138d869939a89",
       "IPY_MODEL_b97b7461cf33481581ce7fe7d746ef3f"
      ],
      "layout": "IPY_MODEL_e0d70b350ce24b618d427907fdd72f9d"
     }
    },
    "1aa4404546d048be9095ce36f9a7cab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "243d56b7c05248af8cd275b4c03d5453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25764dd28af84378be5588b2c2d771d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b441a90c1b948348a5e718b2f20a382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aac6248693ab425db9e5879c9a1705d4",
      "placeholder": "​",
      "style": "IPY_MODEL_d0dd958562304ef89c02216518d2273a",
      "value": "100%"
     }
    },
    "3e028a9817c746ca824b1a21e3fe6b22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "426f35e25c5541f2822f3bc7744bb7b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd3465e5cd6240debfd29b2ecfb3bfe1",
      "placeholder": "​",
      "style": "IPY_MODEL_f7476c58ef3b48f4948443679a1bdf47",
      "value": "100%"
     }
    },
    "5580f604ab664d4ab28b567376804c91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b7c69538d2d48fca6bb0e454b43a896": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8623ad7c0c594f0298c6ede3097fabc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5580f604ab664d4ab28b567376804c91",
      "placeholder": "​",
      "style": "IPY_MODEL_bcab682a4dce4516b002f0272be5e5aa",
      "value": "100%"
     }
    },
    "88420b2c8e2341118d1222fc1c19466d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8896aace47ba49efad33b80b68bf0c86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d3e9aff149d464f9cf51aad6a3463f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8623ad7c0c594f0298c6ede3097fabc9",
       "IPY_MODEL_e70c980936be422fb12f352b6fc0e4e0",
       "IPY_MODEL_10b1cd1eb5054ea4a2dff02443080e0c"
      ],
      "layout": "IPY_MODEL_fdb49bb47e0b4156b1ba9a3cd2aa9caf"
     }
    },
    "9f538f9d8f334ef589bb03ed39c524a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17c8957d90b24023bd4f34b7e69dfaff",
       "IPY_MODEL_ca3b225fe04042c1b11361926faf9c72",
       "IPY_MODEL_9f95feb156ad47b4b196b497057603cf"
      ],
      "layout": "IPY_MODEL_b9199a2f38b743758229ff96587aade7"
     }
    },
    "9f95feb156ad47b4b196b497057603cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb44322fee4549318415abec9c8a8325",
      "placeholder": "​",
      "style": "IPY_MODEL_0807f4e7748e4bf2b0ad2a8587ed4e5a",
      "value": " 1775/1775 [00:05&lt;00:00, 325.75ba/s]"
     }
    },
    "a0c76b6082e5418bb1624dab8fd6f68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aac6248693ab425db9e5879c9a1705d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9199a2f38b743758229ff96587aade7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b97b7461cf33481581ce7fe7d746ef3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e028a9817c746ca824b1a21e3fe6b22",
      "placeholder": "​",
      "style": "IPY_MODEL_ff5a6eb413484323948a15e358a54f78",
      "value": " 91/91 [01:26&lt;00:00,  1.41ba/s]"
     }
    },
    "bb44322fee4549318415abec9c8a8325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb6f3e8b55c4472fbe4de30e13674f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c3127e067f1411ca3efdd36b233cb10",
      "max": 91,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8896aace47ba49efad33b80b68bf0c86",
      "value": 91
     }
    },
    "bcab682a4dce4516b002f0272be5e5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c60e580c0ae7412a82f598ceaf162462": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca3b225fe04042c1b11361926faf9c72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25764dd28af84378be5588b2c2d771d4",
      "max": 1775,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0c76b6082e5418bb1624dab8fd6f68a",
      "value": 1775
     }
    },
    "cd3465e5cd6240debfd29b2ecfb3bfe1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cead868d99864f5e9162e2b4bcc922e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d047ad4add1e435886d586aebecaa2ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d07f9a9e090947b1aaaca87429108fb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0dd958562304ef89c02216518d2273a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de898c4bfdf24aa5966b9c31e913b2f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0d70b350ce24b618d427907fdd72f9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0e87c7d1bb94a1fa42467a101db1f61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e70c980936be422fb12f352b6fc0e4e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d047ad4add1e435886d586aebecaa2ee",
      "max": 198,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fcbbcf6fc964f7f8d7fbf75a453cc68",
      "value": 198
     }
    },
    "f7476c58ef3b48f4948443679a1bdf47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdb49bb47e0b4156b1ba9a3cd2aa9caf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff5a6eb413484323948a15e358a54f78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
